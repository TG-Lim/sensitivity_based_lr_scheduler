{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45857777",
   "metadata": {},
   "source": [
    "### Model 준비과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff824c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 1. 스크립트 최상단에서 랜덤 시드 고정 함수 정의 및 실행\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# 원하는 시드 값으로 설정 (이 숫자를 바꾸지 않는 한 항상 동일하게 초기화됨)\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# 사용할 모델의 Hugging Face 저장소 이름\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc905f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 토크나이저를 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # 사용 가능한 GPU에 모델을 자동으로 할당합니다.\n",
    "    torch_dtype=torch.bfloat16,  # 모델을 float16으로 로드합니다.\n",
    "    trust_remote_code=True,  # 원격 코드 신뢰 설정\n",
    ")\n",
    "\n",
    "# 해당 모델에 맞는 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left',\n",
    "    use_fast=True)\n",
    "\n",
    "print(\"모델과 토크나이저를 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None\n",
    "model.generation_config.top_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7b9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 행렬의 rank (값이 클수록 파라미터 수가 늘어남)\n",
    "    lora_alpha=16,  # LoRA 스케일링 alpha (보통 r의 2배)\n",
    "    # Qwen2 아키텍처의 Attention 관련 레이어를 타겟으로 지정\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.0, # Dropout 비율\n",
    "    bias=\"none\", # bias는 학습하지 않음\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02103835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,490,944 || all params: 1,545,205,248 || trainable%: 0.0965\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 가능한 파라미터 수 출력 (LoRA의 효율성 확인)\n",
    "model.print_trainable_parameters()\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44f4a2",
   "metadata": {},
   "source": [
    "### Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b194f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f267bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # causal language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba4eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b27625b808f44d0b64561451d28fee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9918a12977b4fbc8c9417f74d4cd0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_excel(\"data.xlsx\")\n",
    "df[\"system\"] = df[\"system\"].fillna(\"\")  # system 결측값 처리\n",
    "# 3. Train과 Validation으로 데이터 분할\n",
    "train_df = df.sample(frac=0.8, random_state=42)  # 80% Train\n",
    "val_df = df.drop(train_df.index)  # 나머지 20% Validation\n",
    "\n",
    "def convert_to_chat_format(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "    ]\n",
    "\n",
    "train_chat_data = [convert_to_chat_format(row) for _, row in train_df.iterrows()]\n",
    "train_dataset = Dataset.from_dict({\"conversations\": train_chat_data})\n",
    "\n",
    "def apply_chat_template_to_dataset(example):\n",
    "    # conversations 하나씩 처리\n",
    "    conversation = example[\"conversations\"]\n",
    "    \n",
    "    # apply_chat_template 적용\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,  # 아직 토크나이징하지 않고 텍스트만\n",
    "        add_generation_prompt=False  # 학습용이므로 generation prompt 불필요\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "formatted_dataset = train_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 이제 formatted_text들을 실제로 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,  # 필요에 따라 조정\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels는 input_ids와 동일하게 설정\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 토크나이징 적용\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,  # 배치 단위로 처리 (더 효율적)\n",
    "    remove_columns=formatted_dataset.column_names  # 기존 컬럼들 제거 (text 컬럼 등)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d01d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1576f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954cd7f02f664ee28845fb0de6ea064a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a674b698e954d85836dbf6e77405327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_metric_format(row):\n",
    "    return {\n",
    "        \"input\": [\n",
    "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]}\n",
    "        ],\n",
    "        \"target\": row[\"response\"]  # 정답 assistant 응답\n",
    "    }\n",
    "\n",
    "val_chat_data = [convert_to_chat_format(row) for _, row in val_df.iterrows()]\n",
    "val_dataset = Dataset.from_dict({\"conversations\": val_chat_data})\n",
    "\n",
    "val_metric_data = [create_metric_format(row) for _, row in val_df.iterrows()]\n",
    "val_metric_dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in val_metric_data],\n",
    "    \"target\": [item[\"target\"] for item in val_metric_data]\n",
    "})\n",
    "\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "val_formatted_dataset = val_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "# 토크나이징 적용\n",
    "val_tokenized_dataset = val_formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_formatted_dataset.column_names\n",
    ")\n",
    "\n",
    "# Validation DataLoader 생성\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,  # validation은 섞지 않음\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49c7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfeb00",
   "metadata": {},
   "source": [
    "### 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15a1f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "Total trainable parameters: 168\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        cnt += 1\n",
    "        print(f\"{name} - {param.shape}\")\n",
    "print(f\"Total trainable parameters: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc43f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e315cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepSensitivityScheduler:\n",
    "    \"\"\"\n",
    "    그래디언트 민감도에 기반하여 각 파라미터 그룹의 학습률을 동적으로 조절하는 스케줄러.\n",
    "    '민감도가 높을수록 학습률을 낮추는' 적응적 방식을 사용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer: AdamW, base_lr: float, lr_min: float, lr_max: float):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        # 수정된 로직을 포함한 _make_multipliers 호출\n",
    "        self._make_multipliers()\n",
    "\n",
    "    def _compute_rep_sensitivity(self, param: torch.Tensor, grad: torch.Tensor) -> float:\n",
    "        sens = (param.float() * grad.float()).pow(2)\n",
    "        svals = torch.linalg.svdvals(sens)\n",
    "        return svals.sum().item()\n",
    "\n",
    "    def _make_multipliers(self):\n",
    "        steps = 5\n",
    "        center = 1.0\n",
    "        half = steps // 2\n",
    "        delta_down = (center - self.lr_min) / half\n",
    "        delta_up = (self.lr_max - center) / half\n",
    "        lower = [center - i * delta_down for i in reversed(range(1, half + 1))]\n",
    "        upper = [center + i * delta_up for i in range(1, half + 1)]\n",
    "        \n",
    "        multipliers_tensor = torch.tensor(lower + [center] + upper)\n",
    "        \n",
    "        # [수정 완료] 텐서의 순서를 뒤집어 저장합니다.\n",
    "        self.multipliers = torch.flip(multipliers_tensor, dims=[0])\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"옵티마이저의 파라미터 그룹별로 학습률을 업데이트합니다.\"\"\"\n",
    "        sens_dict = {}\n",
    "        sens_values = []\n",
    "        \n",
    "        all_params = [p for group in self.optimizer.param_groups for p in group['params']]\n",
    "\n",
    "        for p in all_params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            rep = self._compute_rep_sensitivity(p.data, p.grad)\n",
    "            sens_dict[id(p)] = rep\n",
    "            sens_values.append(rep)\n",
    "\n",
    "        if not sens_values:\n",
    "            return\n",
    "\n",
    "        sens_tensor = torch.tensor(sens_values, dtype=torch.float32)\n",
    "        quantile_edges = torch.quantile(sens_tensor, torch.linspace(0, 1, steps=6))\n",
    "        quantile_edges[-1] += 1e-6\n",
    "\n",
    "        # step 메소드의 이 부분은 수정할 필요가 없습니다.\n",
    "        # self.multipliers가 이미 뒤집혀 있으므로 로직이 자동으로 적용됩니다.\n",
    "        for group in self.optimizer.param_groups:\n",
    "            p = group['params'][0]\n",
    "            if id(p) not in sens_dict:\n",
    "                continue\n",
    "            rep = sens_dict[id(p)]\n",
    "            bucket = torch.bucketize(torch.tensor(rep), quantile_edges[1:-1]).item()\n",
    "            multiplier = self.multipliers[bucket].item()\n",
    "            group['lr'] = self.base_lr * multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f61ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR = 1e-5\n",
    "MAX_STEPS = 500\n",
    "EVAL_STEPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "974a38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "param_groups = [{\"params\": [p]} for p in trainable_params]\n",
    "optimizer = AdamW(param_groups, lr=BASE_LR)\n",
    "\n",
    "# 스케줄러 클래스를 초기화합니다.\n",
    "scheduler = RepSensitivityScheduler(optimizer, base_lr=BASE_LR, lr_min=0.2, lr_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afac953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Training Progress: 100%|██████████| 500/500 [17:20<00:00,  2.08s/it, train_loss=0.2545, val_loss=0.2772] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "history = {\n",
    "    \"steps\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(total=MAX_STEPS, desc=\"Overall Training Progress\")\n",
    "\n",
    "while global_step < MAX_STEPS:\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if global_step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # 1. forward\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 2. optimizer step\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # 3. 정해진 스텝마다 평가\n",
    "        if global_step % EVAL_STEPS == 0:\n",
    "            avg_train_loss = loss.item()\n",
    "            history[\"steps\"].append(global_step)\n",
    "            history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "            # --- Validation Loss calculation ---\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_batch = {k: v.to(model.device) for k, v in val_batch.items()}\n",
    "                    val_outputs = model(**val_batch)\n",
    "                    val_loss += val_outputs.loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                train_loss=f\"{avg_train_loss:.4f}\",\n",
    "                val_loss=f\"{avg_val_loss:.4f}\",\n",
    "            )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b517248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp/training_history_my.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(history, \"temp/training_history_my.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba106c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5fbd9007be44579d94d4a56db79f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\KG\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--kyu5787--robot_task_planning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56d72e80af54d5b825b509cf9af7eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kyu5787/robot_task_planning/commit/a303486bc090d7de3f4c652711cf4dd1fc6cedac', commit_message='Upload model', commit_description='', oid='a303486bc090d7de3f4c652711cf4dd1fc6cedac', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kyu5787/robot_task_planning', endpoint='https://huggingface.co', repo_type='model', repo_id='kyu5787/robot_task_planning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('kyu5787/robot_task_planning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e45cbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ebac8ac2f0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67607723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acd0195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [02:50<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "val_preds, val_refs = [], []\n",
    "\n",
    "for item in tqdm(val_metric_dataset):\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        item['input'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    if \"assistant\\n\" in generated_text:\n",
    "        pred = generated_text.split(\"assistant\\n\", 1)[1].strip()\n",
    "    else:\n",
    "        pred = generated_text.strip()  # fallback\n",
    "    \n",
    "    val_preds.append(pred)\n",
    "    val_refs.append(item['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aed1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th val pred: [0.5, 0.0, 0.0] = detect('keyboard')\n",
      "0 th val ref: coords = detect('keyboard')\n",
      "move_to(coords)\n",
      "1 th val pred: coords = detect('pen')\n",
      "move_to(coords)\n",
      "pick_up([coords[0]+5, coords[1], coords[2]])\n",
      "place([coords[0]+5, coords[1], coords[2]])\n",
      "1 th val ref: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 5, coords[1], coords[2]]\n",
      "place(coords)\n",
      "2 th val pred: change_movement_speed(40)\n",
      "move_to([0,0,1.5])\n",
      "2 th val ref: change_movement_speed(40)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2]+0.5]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "3 th val pred: change_movement_speed(45)\n",
      "3 th val ref: change_movement_speed(45)\n",
      "4 th val pred: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords = detect('top shelf')\n",
      "place(coords)\n",
      "4 th val ref: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "5 th val pred: coords = detect('table')\n",
      "coords2 = detect('wall')\n",
      "dist = coords[0] - coords2[0]\n",
      "print(f\"The distance between the table and the wall is {dist} units.\")\n",
      "5 th val ref: coords_table = detect('table')\n",
      "coords_wall = detect('wall')\n",
      "distance = np.sqrt(((coords_table[0]+coords_wall[0])/2)2 + ((coords_table[1]+coords_wall[1])/2)2 + ((coords_table[2]+coords_wall[2])/2)2)\n",
      "print(distance)\n",
      "6 th val pred: coords = detect('left wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "time.sleep(2)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "6 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]+0.25, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(2)\n",
      "move_to(coords)\n",
      "7 th val pred: coords = detect('yellow screw')\n",
      "pick_up(coords)\n",
      "coords = detect('hole on the wall')\n",
      "place(coords)\n",
      "7 th val ref: print('Error: can not use screwer when using gripper')\n",
      "8 th val pred: import numpy as np\n",
      "from capabilities import *\n",
      "\n",
      "def execute_task():\n",
      "    # Move to the first position\n",
      "    move_to([0.5, 0.5, 1])\n",
      "    \n",
      "    # Pick up the green block\n",
      "    pick_up([0.3, 0.4, 0.2])\n",
      "    \n",
      "    # Place the green block on the yellow block\n",
      "    place([0.7, 0.4, 0.2])\n",
      "    \n",
      "    # Rotate the screwer to face the yellow block\n",
      "    rotate_screwer([0.8, 0.6, 0.2])\n",
      "8 th val ref: coords_g = detect('green block')\n",
      "coords_y = detect('yellow block')\n",
      "coords_b = detect('blue block')\n",
      "coords_y = [coords_y[0], coords_y[1], coords_y[2]+0.2]\n",
      "pick_up(coords_g)\n",
      "place(coords_y)\n",
      "pick_up(coords_y)\n",
      "place(coords_g)\n",
      "pick_up(coords_b)\n",
      "place(coords_y)\n",
      "9 th val pred: coords = detect('white ruler')\n",
      "pick_up(coords)\n",
      "coords = detect('small battery')\n",
      "pick_up(coords)\n",
      "place(coords)\n",
      "move_to([0.5, 0.0, 1.0])\n",
      "change_movement_speed(20)\n",
      "coords = detect('white ruler')\n",
      "place(coords)\n",
      "coords = detect('small battery')\n",
      "place(coords)\n",
      "9 th val ref: coords_r = detect('white ruler')\n",
      "coords_b = detect('small battery')\n",
      "coords_b2 = [coords_b[0], coords_b[1]+0.3, coords_b[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_b2)\n",
      "pick_up(coords_b)\n",
      "place(coords_r)\n",
      "pick_up(coords_b2)\n",
      "place(coords_b)\n",
      "10 th val pred: coords = [[0, 0, 0], [3, 0, 0], [-3, 0, 0], [0, -5, 0], [0, 5, 0]]\n",
      "for i in range(len(coords)):\n",
      "    if i == len(coords) - 1:\n",
      "        move_to(coords[i])\n",
      "    else:\n",
      "        move_to(coords[i])\n",
      "        coords.append([coords[i][0] + 1, coords[i][1], coords[i][2]])\n",
      "10 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.03, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.005]\n",
      "move_to(coords)\n",
      "coords = [coords[0]+1, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "11 th val pred: coords = detect('yellow block')\n",
      "move_to(coords)\n",
      "11 th val ref: coords = detect('yellow block')\n",
      "move_to(coords)\n",
      "12 th val pred: coords_blue_bottle = detect('blue bottle')\n",
      "coords_green_shelf = detect('green shelf')\n",
      "\n",
      "move_to(coords_blue_bottle)\n",
      "rotate_screwer([0, 0, 90])\n",
      "pick_up(coords_blue_bottle)\n",
      "place(coords_green_shelf)\n",
      "12 th val ref: coords = detect('blue bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('green shelf')\n",
      "place(coords)\n",
      "13 th val pred: coords = detect('printer')\n",
      "move_to(coords)\n",
      "13 th val ref: coords = detect('printer')\n",
      "move_to(coords)\n",
      "14 th val pred: coords = detect('door')\n",
      "coords2 = detect('window')\n",
      "dist = coords[0] - coords2[0]\n",
      "print(dist)\n",
      "14 th val ref: coords_door = detect('door')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_door[0]+coords_window[0])/2)2 + ((coords_door[1]+coords_window[1])/2)2 + ((coords_door[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "15 th val pred: coords = detect('orange pen')\n",
      "pick_up(coords)\n",
      "coords = detect('blue book')\n",
      "place(coords)\n",
      "15 th val ref: coords = detect('orange pen')\n",
      "pick_up(coords)\n",
      "coords = detect('blue book')\n",
      "coords = [coords[0], coords[1]-0.2, coords[2]]\n",
      "place(coords)\n",
      "16 th val pred: coords = detect('leftmost screw')\n",
      "pick_up(coords)\n",
      "move_to([0, 0, 1])\n",
      "rotate_screwer([0, 0, 90])\n",
      "place(coords)\n",
      "16 th val ref: coords = detect('leftmost screw')\n",
      "rotate_screwer(coords)\n",
      "17 th val pred: coords = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n",
      "for i in range(3):\n",
      "    if i == 0:\n",
      "        coords[i] = detect('right')\n",
      "    elif i == 1:\n",
      "        coords[i] = detect('upwards')\n",
      "    else:\n",
      "        coords[i] = detect('left')\n",
      "move_to(coords[0])\n",
      "change_movement_speed(80)\n",
      "pick_up(coords[1])\n",
      "place(coords[2])\n",
      "17 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.05, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]+0.01]\n",
      "move_to(coords)\n",
      "coords = [coords[0]+2, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "18 th val pred: coords = detect('red block')\n",
      "move_to(coords)\n",
      "18 th val ref: coords = detect('red block')\n",
      "move_to(coords)\n",
      "19 th val pred: coords = detect('green block')\n",
      "move_to(coords)\n",
      "19 th val ref: coords = detect('green block')\n",
      "move_to(coords)\n",
      "20 th val pred: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "20 th val ref: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "21 th val pred: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 15, coords[2]]\n",
      "place(coords)\n",
      "21 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]+0.15]\n",
      "place(coords)\n",
      "22 th val pred: coords = detect('chair')\n",
      "coords2 = detect('desk')\n",
      "pick_up(coords)\n",
      "move_to(coords2)\n",
      "place(coords2)\n",
      "22 th val ref: coords = detect('chair')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "23 th val pred: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('charging station')\n",
      "place(coords)\n",
      "23 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('charging station')\n",
      "place(coords)\n",
      "24 th val pred: coords = detect('screws')\n",
      "pick_up(coords)\n",
      "coords = detect('red screws')\n",
      "place(coords)\n",
      "coords = detect('blue screws')\n",
      "coords = detect('green screws')\n",
      "24 th val ref: coords_g = detect('green screw')\n",
      "coords_r = detect('red screw')\n",
      "coords_b = detect('blue screw')\n",
      "coords_nextto_g = [coords_g[0] - 0.1, coords_g[1], coords_g[2]]\n",
      "coords_nextto_r = [coords_g[0] - 0.2, coords_g[1], coords_g[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_nextto_g)\n",
      "pick_up(coords_b)\n",
      "place(coords_nextto_r)\n",
      "25 th val pred: coords = detect('yellow box')\n",
      "move_to([coords[0], coords[1] - 1, coords[2]])\n",
      "25 th val ref: coords = detect('yellow box')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1]-1, coords[2]]\n",
      "place(coords)\n",
      "26 th val pred: coords = detect('blue mat')\n",
      "move_to(coords)\n",
      "26 th val ref: coords = detect('blue mat')\n",
      "move_to(coords)\n",
      "27 th val pred: coords = detect('left')\n",
      "pick_up(coords)\n",
      "coords = detect('down')\n",
      "place(coords)\n",
      "27 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.10, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.3]\n",
      "move_to(coords)\n",
      "28 th val pred: coords = detect('red bin')\n",
      "move_to(coords)\n",
      "28 th val ref: coords = detect('red bin')\n",
      "move_to(coords)\n",
      "29 th val pred: coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "move_to([coords[0]-0.05, coords[1], coords[2]])\n",
      "place(coords)\n",
      "29 th val ref: coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "coords = [coords[0]+0.05, coords[1], coords[2]]\n",
      "place(coords)\n",
      "30 th val pred: coords = detect('block')\n",
      "move_to(coords)\n",
      "30 th val ref: coords = detect('block')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "31 th val pred: coords = detect('block')\n",
      "move_to(coords)\n",
      "31 th val ref: coords = detect('block')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "32 th val pred: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('reading shelf')\n",
      "place(coords)\n",
      "32 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('reading shelf')\n",
      "place(coords)\n",
      "33 th val pred: coords1 = detect('desk')\n",
      "coords2 = detect('lamp')\n",
      "distance = coords2[0] - coords1[0]\n",
      "print(distance)\n",
      "33 th val ref: coords_desk = detect('desk')\n",
      "coords_lamp = detect('lamp')\n",
      "distance = np.sqrt(((coords_desk[0]+coords_lamp[0])/2)2 + ((coords_desk[1]+coords_lamp[1])/2)2 + ((coords_desk[2]+coords_lamp[2])/2)2)\n",
      "print(distance)\n",
      "34 th val pred: coords = detect('books')\n",
      "colors = ['red', 'blue', 'green']\n",
      "sorted_coords = []\n",
      "for color in colors:\n",
      "    filtered_coords = [coord for coord in coords if coord[2] == color]\n",
      "    sorted_coords.append(filtered_coords)\n",
      "move_to(sorted_coords[0][0])\n",
      "rotate_screwer(sorted_coords[0][1])\n",
      "pick_up(sorted_coords[0][2])\n",
      "place(sorted_coords[1][0])\n",
      "rotate_screwer(sorted_coords[1][1])\n",
      "pick_up(sorted_coords[1][2])\n",
      "place(sorted_coords[2][0])\n",
      "rotate_screwer(sorted_coords[2][1])\n",
      "pick\n",
      "34 th val ref: coords_r = detect('red book')\n",
      "coords_g = detect('green book')\n",
      "coords_b = detect('blue book')\n",
      "coords_nextto_r = [coords_r[0] - 0.1, coords_r[1], coords_r[2]]\n",
      "coords_nextto_g = [coords_r[0] - 0.2, coords_r[1], coords_r[2]]\n",
      "pick_up(coords_g)\n",
      "place(coords_nextto_r)\n",
      "pick_up(coords_b)\n",
      "place(coords_nextto_g)\n",
      "35 th val pred: coords = detect('yellow ruler')\n",
      "pick_up(coords)\n",
      "coords = detect('blue notebook')\n",
      "place(coords)\n",
      "35 th val ref: coords = detect('yellow ruler')\n",
      "pick_up(coords)\n",
      "coords = detect('blue notebook')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "36 th val pred: coords = detect('laptop')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "36 th val ref: coords = detect('laptop')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "37 th val pred: coords = detect('ruler')\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "37 th val ref: coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "38 th val pred: [0.5, 0.5, 0.5] = detect('red')\n",
      "38 th val ref: coords_y = detect('yellow block')\n",
      "coords_g = detect('green block')\n",
      "coords_b = detect('blue block')\n",
      "coords_y = [coords_y[0], coords_y[1], coords_y[2]+0.2]\n",
      "pick_up(coords_y)\n",
      "place(coords_g)\n",
      "pick_up(coords_g)\n",
      "place(coords_y)\n",
      "pick_up(coords_b)\n",
      "place(coords_g)\n",
      "39 th val pred: coords = detect('green pencil')\n",
      "move_to(coords)\n",
      "39 th val ref: coords = detect('rear green pencil')\n",
      "move_to(coords)\n",
      "40 th val pred: coords = detect('grey bin')\n",
      "move_to(coords)\n",
      "40 th val ref: coords = detect('grey bin')\n",
      "move_to(coords)\n",
      "41 th val pred: coords = detect('red pencil')\n",
      "pick_up(coords)\n",
      "coords = detect('blue battery')\n",
      "pick_up(coords)\n",
      "place(coords)\n",
      "coords = detect('red pencil')\n",
      "place(coords)\n",
      "coords = detect('blue battery')\n",
      "41 th val ref: coords_r = detect('red pencil')\n",
      "coords_b = detect('blue battery')\n",
      "coords_b2 = [coords_b[0], coords_b[1]+0.3, coords_b[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_b2)\n",
      "pick_up(coords_b)\n",
      "place(coords_r)\n",
      "pick_up(coords_b2)\n",
      "place(coords_b)\n",
      "42 th val pred: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "move_to([0,0,-0.5])\n",
      "place(coords)\n",
      "42 th val ref: coords = detect('smallest pencil')\n",
      "pick_up(coords)\n",
      "move_to([coords[0],coords[1],coords[2]+0.5])\n",
      "43 th val pred: coords = detect('red block')\n",
      "coords2 = detect('blue block')\n",
      "pick_up(coords)\n",
      "move_to(coords2)\n",
      "place(coords2)\n",
      "43 th val ref: coords = detect('red block')\n",
      "pick_up(coords)\n",
      "coords = detect('blue block')\n",
      "coords = [coords[0], coords[1], coords[2]-0.2]\n",
      "place(coords)\n",
      "44 th val pred: coords = detect('left')\n",
      "pick_up(coords)\n",
      "coords = detect('up')\n",
      "move_to(coords)\n",
      "44 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.02, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]+0.5]\n",
      "move_to(coords)\n",
      "45 th val pred: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = detect('notebook')\n",
      "place(coords)\n",
      "45 th val ref: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = [coords[0]-0.2, coords[1], coords[2]]\n",
      "place(coords)\n",
      "46 th val pred: coords = detect('chair')\n",
      "coords2 = detect('table')\n",
      "pick_up(coords)\n",
      "move_to(coords2)\n",
      "place(coords2)\n",
      "46 th val ref: coords = detect('green bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('red shelf')\n",
      "place(coords)\n",
      "47 th val pred: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "47 th val ref: coords = detect('chair')\n",
      "move_to(coords)\n",
      "coords_table = detect('table')\n",
      "coords = [coords_table[0]+0.1, coords_table[1], coords_table[2]]\n",
      "place(coords)\n",
      "48 th val pred: coords = detect('screws')\n",
      "pick_up(coords)\n",
      "coords = detect('red screws')\n",
      "place(coords)\n",
      "coords = detect('blue screws')\n",
      "coords = detect('green screws')\n",
      "48 th val ref: coords_door = detect('door')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_door[0]+coords_window[0])/2)2 + ((coords_door[1]+coords_window[1])/2)2 + ((coords_door[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "49 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('door')\n",
      "place(coords)\n",
      "49 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.20, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "coords2 = [coords[0]+0.15, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "50 th val pred: coords = detect('middle screw')\n",
      "pick_up(coords)\n",
      "coords = detect('right side')\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "50 th val ref: coords = detect('screw closest to door')\n",
      "rotate_screwer(coords)\n",
      "51 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = coords[0] + [0.15, 0, 0]\n",
      "place(coords)\n",
      "51 th val ref: coords = detect('middle screw on right')\n",
      "rotate_screwer(coords)\n",
      "52 th val pred: coords = detect('painting')\n",
      "move_to(coords)\n",
      "52 th val ref: coords = detect('first screw on left')\n",
      "rotate_screwer(coords)\n",
      "53 th val pred: coords = detect('lamp')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "53 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('nightstand')\n",
      "place(coords)\n",
      "54 th val pred: coords = detect('black shirt')\n",
      "pick_up(coords)\n",
      "move_to([coords[0]+1, coords[1], coords[2]])\n",
      "place(coords)\n",
      "54 th val ref: coords = detect('lamp')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "55 th val pred: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords = detect('top shelf')\n",
      "place(coords)\n",
      "55 th val ref: coords = detect('pillow')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 0.8, coords[1], coords[2] + 0.3]\n",
      "place(coords)\n",
      "56 th val pred: coords1 = detect('chair')\n",
      "coords2 = detect('lamp')\n",
      "distance = coords2[0] - coords1[0]\n",
      "print(f\"The distance between the chair and the lamp is {distance} units.\")\n",
      "56 th val ref: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "57 th val pred: coords = detect('green umbrella')\n",
      "move_to(coords)\n",
      "57 th val ref: coords = detect('green umbrella')\n",
      "move_to(coords)\n",
      "58 th val pred: coords = detect('left wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "time.sleep(2)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "58 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]+0.4, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(2)\n",
      "move_to(coords)\n",
      "59 th val pred: coords_blue_bottle = detect('blue bottle')\n",
      "coords_green_shelf = detect('green shelf')\n",
      "\n",
      "move_to(coords_blue_bottle)\n",
      "rotate_screwer([0, 0, 90])\n",
      "pick_up(coords_blue_bottle)\n",
      "place(coords_green_shelf)\n",
      "59 th val ref: coords = detect('blue bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('green shelf')\n",
      "place(coords)\n",
      "60 th val pred: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "60 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "61 th val pred: coords = detect('blue box')\n",
      "pick_up(coords)\n",
      "coords = detect('green shelf')\n",
      "place(coords)\n",
      "61 th val ref: coords = detect('blue box')\n",
      "coords_shelf = detect('green shelf')\n",
      "place(coords_shelf)\n",
      "62 th val pred: coords = detect('black bottle')\n",
      "coords2 = detect('red box')\n",
      "pick_up(coords)\n",
      "move_to(coords2)\n",
      "place(coords2)\n",
      "62 th val ref: coords = detect('black bottle')\n",
      "coords_box = detect('red box')\n",
      "coords = [coords_box[0]-0.2, coords_box[1], coords_box[2]]\n",
      "place(coords)\n",
      "63 th val pred: coords = detect('large hammer')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "63 th val ref: coords = detect('large hammer')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('shelf')\n",
      "place(coords_shelf)\n",
      "64 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('wall')\n",
      "place(coords)\n",
      "64 th val ref: coords = detect('screw closest to wall')\n",
      "rotate_screwer(coords)\n",
      "65 th val pred: coords = detect('pencil')\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "65 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "66 th val pred: coords = detect('station 2')\n",
      "move_to(coords)\n",
      "66 th val ref: coords = detect('robotic arm')\n",
      "pick_up(coords)\n",
      "coords = detect('station 2')\n",
      "place(coords)\n",
      "67 th val pred: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red bin')\n",
      "place(coords)\n",
      "67 th val ref: coords = detect('right pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red bin')\n",
      "place(coords)\n",
      "68 th val pred: change_movement_speed(50)\n",
      "move_to([0,0,-1])\n",
      "68 th val ref: change_movement_speed(50)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2] - 1]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "69 th val pred: coords = detect('printer')\n",
      "move_to(coords)\n",
      "69 th val ref: coords = detect('printer')\n",
      "move_to(coords)\n",
      "70 th val pred: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red block')\n",
      "place(coords)\n",
      "70 th val ref: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red block')\n",
      "coords = [coords[0], coords[1] + 0.2, coords[2]]\n",
      "place(coords)\n",
      "71 th val pred: coords = detect('second largest screw')\n",
      "rotate_screwer(coords)\n",
      "71 th val ref: coords = detect('second largest screw')\n",
      "rotate_screwer(coords)\n",
      "72 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('third smallest screw', 'right side')\n",
      "place(coords)\n",
      "72 th val ref: coords = detect('third smallest screw on right')\n",
      "rotate_screwer(coords)\n",
      "73 th val pred: coords = detect('door')\n",
      "move_to(coords)\n",
      "73 th val ref: coords = detect('door')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "74 th val pred: coords = detect('cup')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "pick_up(coords)\n",
      "coords = coords[0] + 1\n",
      "place([coords[0], coords[1], coords[2]])\n",
      "74 th val ref: coords = detect('cup')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "75 th val pred: coords = detect('station 1')\n",
      "move_to(coords)\n",
      "75 th val ref: coords = detect('robotic arm')\n",
      "pick_up(coords)\n",
      "coords = detect('station 1')\n",
      "place(coords)\n",
      "76 th val pred: coords = detect('nail')\n",
      "wrench_coords = detect('wrench')\n",
      "hammer_coords = detect('hammer')\n",
      "\n",
      "move_to(wrench_coords)\n",
      "rotate_screwer(wrench_coords)\n",
      "pick_up(coords)\n",
      "place(hammer_coords)\n",
      "76 th val ref: coords = detect('nail')\n",
      "coords_w = detect('wrench')\n",
      "coords_h = detect('hammer')\n",
      "coords = [(coords_w[0] + coords_h[0]) / 2, (coords_w[1] + coords_h[1]) / 2, (coords_w[2] + coords_h[2]) / 2]\n",
      "place(coords)\n",
      "77 th val pred: coords = detect('wrench')\n",
      "pick_up(coords)\n",
      "77 th val ref: coords = detect('wrench')\n",
      "print(coords)\n",
      "78 th val pred: coords = detect('right')\n",
      "move_to(coords)\n",
      "78 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] - 0.02, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "79 th val pred: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 2, coords[1], coords[2]]\n",
      "coords = [coords[0], coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "79 th val ref: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 2, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "80 th val pred: coords = detect('front green pen')\n",
      "move_to(coords)\n",
      "80 th val ref: coords = detect('green pen')\n",
      "move_to(coords)\n",
      "81 th val pred: coords = detect('red phone')\n",
      "pick_up(coords)\n",
      "coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "move_to([0, 0, 1])\n",
      "change_movement_speed(50)\n",
      "rotate_screwer([0, 0, 0])\n",
      "move_to([0, 0, -1])\n",
      "change_movement_speed(100)\n",
      "place([0, 0, 0])\n",
      "81 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "82 th val pred: coords = detect('monitor')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 3, coords[1], coords[2]]\n",
      "place(coords)\n",
      "82 th val ref: coords = detect('monitor')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 3, coords[1] + 2, coords[2]]\n",
      "place(coords)\n",
      "83 th val pred: coords = detect('smallest bolt')\n",
      "rotate_screwer(coords)\n",
      "83 th val ref: coords = detect('smallest bolt')\n",
      "rotate_screwer(coords)\n",
      "84 th val pred: coords = detect('shelf')\n",
      "move_to(coords)\n",
      "84 th val ref: coords = detect('shelf')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "85 th val pred: coords = detect('tablet')\n",
      "pick_up(coords)\n",
      "coords = detect('white bin')\n",
      "place(coords)\n",
      "85 th val ref: coords = detect('right tablet')\n",
      "pick_up(coords)\n",
      "coords = detect('white bin')\n",
      "place(coords)\n",
      "86 th val pred: coords = detect('stapler')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "move_to(coords)\n",
      "coords = detect('drawer')\n",
      "place(coords)\n",
      "86 th val ref: coords = detect('stapler on shelf')\n",
      "pick_up(coords)\n",
      "coords = detect('drawer')\n",
      "place(coords)\n",
      "87 th val pred: coords = detect('left')\n",
      "move_to(coords)\n",
      "87 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] + 0.03, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "88 th val pred: coords = detect('wall')\n",
      "move_to(coords)\n",
      "88 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "89 th val pred: coords = detect('projector')\n",
      "move_to(coords)\n",
      "89 th val ref: coords = detect('projector')\n",
      "move_to(coords)\n",
      "90 th val pred: coords = detect('glass')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "90 th val ref: coords = detect('glass')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "91 th val pred: coords = detect('rear blue pen')\n",
      "move_to(coords)\n",
      "91 th val ref: coords = detect('rear blue pen')\n",
      "move_to(coords)\n",
      "92 th val pred: coords = detect('cup')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 2, coords[1], coords[2]]\n",
      "place(coords)\n",
      "92 th val ref: coords = detect('cup')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 2, coords[1], coords[2]]\n",
      "place(coords)\n",
      "93 th val pred: coords = detect('red screw')\n",
      "pick_up(coords)\n",
      "coords = detect('far right')\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "93 th val ref: coords = detect('right red screw')\n",
      "rotate_screwer(coords)\n",
      "94 th val pred: coords = detect('screwdriver')\n",
      "pick_up(coords)\n",
      "coords = detect('bottle')\n",
      "move_to(coords)\n",
      "coords = detect('box')\n",
      "place(coords)\n",
      "94 th val ref: coords = detect('screwdriver')\n",
      "coords_b = detect('bottle')\n",
      "coords_s = detect('box')\n",
      "coords = [(coords_b[0]+coords_s[0])/2, (coords_b[1]+coords_s[1])/2, (coords_b[2]+coords_s[2])/2]\n",
      "place(coords)\n",
      "95 th val pred: coords = detect('hammer')\n",
      "move_to(coords)\n",
      "95 th val ref: coords = detect('hammer')\n",
      "print(coords)\n",
      "96 th val pred: coords = detect('can')\n",
      "pick_up(coords)\n",
      "coords = detect('recycling bin')\n",
      "place(coords)\n",
      "96 th val ref: coords = detect('can')\n",
      "pick_up(coords)\n",
      "coords = detect('recycling bin')\n",
      "place(coords)\n",
      "97 th val pred: coords = detect('front yellow pen')\n",
      "move_to(coords)\n",
      "97 th val ref: coords = detect('yellow pen')\n",
      "move_to(coords)\n",
      "98 th val pred: coords = detect('dark drawer')\n",
      "move_to(coords)\n",
      "98 th val ref: coords = detect('dark drawer')\n",
      "move_to(coords)\n",
      "99 th val pred: coords = detect('right wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "time.sleep(3)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "99 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.3, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "move_to(coords)\n",
      "100 th val pred: [0.5, 0.5, 0.5] = detect('small green button')\n",
      "100 th val ref: coords = detect('small green button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.3])\n",
      "change_movement_speed(50)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.3])\n",
      "101 th val pred: coords = detect('blue marker')\n",
      "pick_up(coords)\n",
      "101 th val ref: coords = detect('blue marker')\n",
      "pick_up(coords)\n",
      "102 th val pred: coords = detect('front door')\n",
      "move_to(coords)\n",
      "102 th val ref: coords = detect('front door')\n",
      "move_to(coords)\n",
      "103 th val pred: coords = detect('bolt')\n",
      "pick_up(coords)\n",
      "coords = detect('bottom')\n",
      "place(coords)\n",
      "103 th val ref: coords = detect('bolt near bottom')\n",
      "rotate_screwer(coords)\n",
      "104 th val pred: coords = detect('whiteboard')\n",
      "move_to(coords)\n",
      "104 th val ref: coords = detect('whiteboard')\n",
      "move_to(coords)\n",
      "105 th val pred: coords = detect('chair')\n",
      "coords2 = detect('door')\n",
      "dist = coords[0] - coords2[0]\n",
      "print(dist)\n",
      "105 th val ref: coords_c = detect('chair')\n",
      "coords_d = detect('door')\n",
      "distance = np.sqrt(((coords_c[0] + coords_d[0]) / 2)  2 + ((coords_c[1] + coords_d[1]) / 2)  2 + ((coords_c[2] + coords_d[2]) / 2)  2)\n",
      "print(distance)\n",
      "106 th val pred: coords = detect('right wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "time.sleep(3)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "106 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0] - 0.1, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "move_to(coords)\n",
      "107 th val pred: coords = detect('wall')\n",
      "move_to(coords)\n",
      "107 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.15, coords[2]]\n",
      "move_to(coords)\n",
      "108 th val pred: coords = detect('red pen')\n",
      "coords2 = detect('blue block')\n",
      "pick_up(coords)\n",
      "move_to(coords2)\n",
      "place(coords2)\n",
      "108 th val ref: coords = detect('red pen')\n",
      "pick_up(coords)\n",
      "coords = detect('blue block')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "109 th val pred: coords = detect('screws')\n",
      "pick_up(coords)\n",
      "coords = coords[0]\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "109 th val ref: coords = detect('largest screw on left')\n",
      "rotate_screwer(coords)\n",
      "110 th val pred: coords = detect('blue screw')\n",
      "pick_up(coords)\n",
      "coords = detect('hole on the table')\n",
      "place(coords)\n",
      "110 th val ref: print('Error: can not use screwer when using gripper')\n",
      "111 th val pred: coords = detect('small blue button')\n",
      "move_to(coords)\n",
      "pick_up(coords)\n",
      "rotate_screwer([0, 0, 90])\n",
      "place([0, 0, 150])\n",
      "111 th val ref: coords = detect('small blue button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.1])\n",
      "change_movement_speed(50)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.1])\n",
      "112 th val pred: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('top shelf')\n",
      "place(coords)\n",
      "112 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "113 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('right side')\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "113 th val ref: coords = detect('right screw')\n",
      "rotate_screwer(coords)\n",
      "114 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('table')\n",
      "place(coords)\n",
      "114 th val ref: coords_s = detect('small screw')\n",
      "coords_m = detect('medium screw')\n",
      "coords_l = detect('large screw')\n",
      "coords_nextto_s = [coords_s[0] - 0.1, coords_s[1], coords_s[2]]\n",
      "coords_nextto_m = [coords_s[0] - 0.2, coords_s[1], coords_s[2]]\n",
      "pick_up(coords_m)\n",
      "place(coords_nextto_s)\n",
      "pick_up(coords_l)\n",
      "place(coords_nextto_m)\n",
      "115 th val pred: coords = detect('small block')\n",
      "pick_up(coords)\n",
      "coords = detect('yellow box')\n",
      "place(coords)\n",
      "115 th val ref: coords = detect('small block')\n",
      "pick_up(coords)\n",
      "coords = detect('yellow box')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "116 th val pred: coords = detect('table')\n",
      "coords[0] += 1\n",
      "coords[2] -= 0.5\n",
      "pick_up(coords)\n",
      "coords = detect('table')\n",
      "coords[0] -= 1\n",
      "coords[2] += 0.5\n",
      "place(coords)\n",
      "116 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] - 1, coords[1], coords[2] - 0.5]\n",
      "move_to(coords)\n",
      "117 th val pred: change_movement_speed(30)\n",
      "117 th val ref: change_movement_speed(30)\n",
      "118 th val pred: coords = detect('monitor')\n",
      "move_to(coords)\n",
      "118 th val ref: coords = detect('monitor')\n",
      "coords = [coords[0], coords[1]-0.5, coords[2]]\n",
      "move_to(coords)\n",
      "119 th val pred: coords = detect('bright yellow box')\n",
      "move_to(coords)\n",
      "119 th val ref: coords = detect('bright yellow box')\n",
      "move_to(coords)\n",
      "120 th val pred: [0.5, 0.5, 0.5] = detect('wall')\n",
      "120 th val ref: coords_table = detect('table')\n",
      "coords_wall = detect('wall')\n",
      "distance = np.sqrt(((coords_table[0]+coords_wall[0])/2)2 + ((coords_table[1]+coords_wall[1])/2)2 + ((coords_table[2]+coords_wall[2])/2)2)\n",
      "print(distance)\n",
      "121 th val pred: coords = detect('green pen')\n",
      "pick_up(coords)\n",
      "coords = detect('rear dark pencil')\n",
      "place(coords)\n",
      "121 th val ref: coords = detect('green pen')\n",
      "pick_up(coords)\n",
      "coords = detect('rear dark pencil')\n",
      "place(coords)\n",
      "122 th val pred: change_movement_speed(70)\n",
      "move_to([0,0,0.2])\n",
      "122 th val ref: change_movement_speed(70)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "123 th val pred: coords = detect('desk')\n",
      "coords2 = detect('window')\n",
      "distance = coords[0] - coords2[0]\n",
      "print(distance)\n",
      "123 th val ref: coords_desk = detect('desk')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_desk[0]+coords_window[0])/2)2 + ((coords_desk[1]+coords_window[1])/2)2 + ((coords_desk[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "124 th val pred: coords = detect('clock')\n",
      "pick_up(coords)\n",
      "coords = detect('wall')\n",
      "place(coords)\n",
      "124 th val ref: coords = detect('clock')\n",
      "pick_up(coords)\n",
      "coords_wall = detect('wall')\n",
      "place(coords_wall)\n",
      "125 th val pred: coords = detect('jacket')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 1.5, coords[1], coords[2]]\n",
      "place(coords)\n",
      "125 th val ref: coords = detect('jacket')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 1.5, coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "126 th val pred: coords = detect('large blue button')\n",
      "move_to(coords)\n",
      "pick_up(coords)\n",
      "rotate_screwer([0, 0, -90])\n",
      "place([0, 0, 150])\n",
      "126 th val ref: coords = detect('large blue button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.08])\n",
      "change_movement_speed(70)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.08])\n",
      "127 th val pred: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('right blue bin')\n",
      "place(coords)\n",
      "127 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('right blue bin')\n",
      "place(coords)\n",
      "128 th val pred: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = detect('floor')\n",
      "place(coords)\n",
      "128 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.5]\n",
      "place(coords)\n",
      "129 th val pred: coords = detect('black phone')\n",
      "pick_up(coords)\n",
      "coords = detect('trash')\n",
      "place(coords)\n",
      "129 th val ref: coords = detect('black phone')\n",
      "pick_up(coords)\n",
      "coords = detect('trash')\n",
      "place(coords)\n",
      "130 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('right side')\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "130 th val ref: coords = detect('largest screw on right')\n",
      "rotate_screwer(coords)\n",
      "131 th val pred: coords = detect('red pen')\n",
      "move_to(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "coords = detect('red pen')\n",
      "pick_up(coords)\n",
      "coords = coords[0] + 1, coords[1], coords[2]\n",
      "place(coords)\n",
      "131 th val ref: coords = detect('red pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 1, coords[1], coords[2]]\n",
      "place(coords)\n",
      "132 th val pred: coords = detect('hammer')\n",
      "box_coords = detect('box')\n",
      "screwdriver_coords = detect('screwdriver')\n",
      "\n",
      "move_to(box_coords)\n",
      "rotate_screwer(screwdriver_coords)\n",
      "pick_up(coords)\n",
      "place(coords)\n",
      "132 th val ref: coords = detect('hammer')\n",
      "coords_b = detect('box')\n",
      "coords_s = detect('screwdriver')\n",
      "coords = [(coords_b[0]+coords_s[0])/2, (coords_b[1]+coords_s[1])/2, (coords_b[2]+coords_s[2])/2]\n",
      "place(coords)\n",
      "133 th val pred: coords = detect('back')\n",
      "move_to(coords)\n",
      "133 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]-0.08, coords[2]]\n",
      "move_to(coords)\n",
      "134 th val pred: coords = detect('pizza')\n",
      "pick_up(coords)\n",
      "coords = detect('oven')\n",
      "place(coords)\n",
      "134 th val ref: coords = detect('pizza')\n",
      "pick_up(coords)\n",
      "coords = detect('oven')\n",
      "place(coords)\n",
      "135 th val pred: coords = detect('cabinet')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "pick_up(coords)\n",
      "coords = detect('cabinet bottom')\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "place(coords)\n",
      "135 th val ref: coords_cabinet = detect('cabinet')\n",
      "depth = coords_cabinet[2]\n",
      "print(depth)\n",
      "136 th val pred: coords = detect('brown bin')\n",
      "move_to(coords)\n",
      "136 th val ref: coords = detect('brown bin')\n",
      "move_to(coords)\n",
      "137 th val pred: coords = detect('curtains')\n",
      "coords2 = detect('window')\n",
      "move_to(coords)\n",
      "rotate_screwer(coords2)\n",
      "137 th val ref: coords = detect('curtains')\n",
      "pick_up(coords)\n",
      "coords_window = detect('window')\n",
      "place(coords_window)\n",
      "138 th val pred: coords = detect('carpet')\n",
      "coords[0] -= 1.5\n",
      "coords[2] += 1\n",
      "pick_up(coords)\n",
      "coords[0] += 3\n",
      "coords[2] -= 3\n",
      "place(coords)\n",
      "138 th val ref: coords = detect('carpet')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 1.5, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "139 th val pred: coords = detect('shelf')\n",
      "pick_up(coords)\n",
      "coords = detect('floor')\n",
      "place(coords)\n",
      "139 th val ref: coords_shelf = detect('shelf')\n",
      "pick_up(coords_shelf)\n",
      "coords_floor = detect('floor')\n",
      "place(coords_floor)\n",
      "140 th val pred: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('green block')\n",
      "place(coords)\n",
      "140 th val ref: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('green block')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "141 th val pred: coords = detect('front yellow pen')\n",
      "move_to(coords)\n",
      "141 th val ref: coords = detect('yellow pen')\n",
      "move_to(coords)\n",
      "142 th val pred: coords = detect('gray bin')\n",
      "move_to(coords)\n",
      "142 th val ref: coords = detect('gray bin')\n",
      "move_to(coords)\n",
      "143 th val pred: coords = detect('green screw')\n",
      "pick_up(coords)\n",
      "coords = detect('hole on the wall')\n",
      "place(coords)\n",
      "143 th val ref: print('Error: can not use screwer when using gripper')\n",
      "144 th val pred: coords = detect('large red button')\n",
      "move_to(coords)\n",
      "pick_up(coords)\n",
      "rotate_screwer([0, 0, -90])\n",
      "place([0, 0, 150])\n",
      "144 th val ref: coords = detect('large red button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.06])\n",
      "change_movement_speed(80)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.06])\n",
      "145 th val pred: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "145 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "146 th val pred: coords = detect('scissors')\n",
      "pick_up(coords)\n",
      "coords = detect('table')\n",
      "place(coords)\n",
      "146 th val ref: coords = detect('scissors')\n",
      "pick_up(coords)\n",
      "coords = detect('table')\n",
      "place(coords)\n",
      "147 th val pred: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "move_to([coords[0]-0.1, coords[1], coords[2]])\n",
      "place([coords[0]-0.1, coords[1], coords[2]])\n",
      "147 th val ref: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 0.1, coords[1], coords[2]]\n",
      "place(coords)\n",
      "148 th val pred: coords = detect('pencil')\n",
      "move_to(coords)\n",
      "148 th val ref: coords = detect('pencil')\n",
      "print(coords)\n",
      "149 th val pred: coords = detect('right')\n",
      "move_to(coords)\n",
      "149 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.05, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "150 th val pred: coords = detect('remote control')\n",
      "pick_up(coords)\n",
      "change_movement_speed(50)\n",
      "coords = [coords[0], coords[1] + 2, coords[2]]\n",
      "coords = [coords[0], coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "150 th val ref: coords = detect('remote control')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 2, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "151 th val pred: coords = detect('black pencil')\n",
      "move_to(coords)\n",
      "151 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "152 th val pred: coords = detect('green cabinet')\n",
      "move_to(coords)\n",
      "152 th val ref: coords = detect('green cabinet')\n",
      "move_to(coords)\n",
      "153 th val pred: coords = detect('glue')\n",
      "move_to(coords)\n",
      "153 th val ref: coords = detect('glue')\n",
      "print(coords)\n",
      "154 th val pred: change_movement_speed(80)\n",
      "move_to([0,0,1.5])\n",
      "154 th val ref: change_movement_speed(80)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+2, coords[2]]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "155 th val pred: coords = detect('left wall')\n",
      "move_to(coords)\n",
      "155 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.1, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "156 th val pred: coords = detect('wall')\n",
      "move_to(coords)\n",
      "156 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.4, coords[2]]\n",
      "move_to(coords)\n",
      "157 th val pred: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "157 th val ref: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "158 th val pred: coords = detect('camera')\n",
      "new_coords = [coords[0] + 3, coords[1] - 1, coords[2]]\n",
      "place(new_coords)\n",
      "158 th val ref: coords = detect('camera')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 3, coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "159 th val pred: coords = detect('front blue pen')\n",
      "move_to(coords)\n",
      "159 th val ref: coords = detect('blue pen')\n",
      "move_to(coords)\n",
      "160 th val pred: coords = detect('rear red pencil')\n",
      "move_to(coords)\n",
      "160 th val ref: coords = detect('rear red pencil')\n",
      "move_to(coords)\n",
      "161 th val pred: coords = detect('third smallest screw')\n",
      "pick_up(coords)\n",
      "rotate_screwer(coords)\n",
      "place(coords)\n",
      "161 th val ref: coords = detect('third smallest screw')\n",
      "rotate_screwer(coords)\n",
      "162 th val pred: coords = detect('black folder')\n",
      "pick_up(coords)\n",
      "coords = detect('stapler')\n",
      "pick_up(coords)\n",
      "move_to([0, 0, -1])\n",
      "place([0, 0, -1])\n",
      "162 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "163 th val pred: coords = detect('front orange pen')\n",
      "move_to(coords)\n",
      "163 th val ref: coords = detect('orange pen')\n",
      "move_to(coords)\n",
      "164 th val pred: coords = detect('right wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(50)\n",
      "time.sleep(4)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "164 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.4, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(4)\n",
      "move_to(coords)\n",
      "165 th val pred: coords = detect('eraser')\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "165 th val ref: coords = detect('eraser')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "166 th val pred: coords = detect('orange block')\n",
      "pick_up(coords)\n",
      "coords = detect('blue cube')\n",
      "place(coords)\n",
      "166 th val ref: coords = detect('orange block')\n",
      "pick_up(coords)\n",
      "coords = detect('blue cube')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "167 th val pred: change_movement_speed(85)\n",
      "167 th val ref: change_movement_speed(85)\n",
      "168 th val pred: coords = detect('window')\n",
      "move_to(coords)\n",
      "168 th val ref: coords = detect('window')\n",
      "move_to(coords)\n",
      "169 th val pred: coords = detect('chair')\n",
      "pick_up(coords)\n",
      "coords = detect('balcony')\n",
      "place(coords)\n",
      "169 th val ref: coords = detect('chair')\n",
      "pick_up(coords)\n",
      "coords = detect('balcony')\n",
      "place(coords)\n",
      "170 th val pred: coords = detect('front purple pen')\n",
      "move_to(coords)\n",
      "170 th val ref: coords = detect('purple pen')\n",
      "move_to(coords)\n",
      "171 th val pred: coords = detect('printer')\n",
      "pick_up(coords)\n",
      "coords = detect('office')\n",
      "place(coords)\n",
      "171 th val ref: coords = detect('printer')\n",
      "pick_up(coords)\n",
      "coords = detect('office')\n",
      "place(coords)\n",
      "172 th val pred: coords = detect('wardrobe')\n",
      "pick_up(coords)\n",
      "coords = detect('corner')\n",
      "place(coords)\n",
      "172 th val ref: coords_wardrobe = detect('wardrobe')\n",
      "coords_corner = detect('corner')\n",
      "place(coords_corner)\n",
      "173 th val pred: coords1 = detect('bathtub')\n",
      "coords2 = detect('shower')\n",
      "distance = coords2[0] - coords1[0]\n",
      "print(f\"The distance between the bathtub and the shower is {distance} units.\")\n",
      "173 th val ref: coords_bathtub = detect('bathtub')\n",
      "coords_shower = detect('shower')\n",
      "distance = np.sqrt(((coords_bathtub[0]+coords_shower[0])/2)2 + ((coords_bathtub[1]+coords_shower[1])/2)2 + ((coords_bathtub[2]+coords_shower[2])/2)2)\n",
      "print(distance)\n",
      "174 th val pred: coords = detect('black pencil')\n",
      "move_to(coords)\n",
      "174 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "175 th val pred: coords = detect('right wall')\n",
      "move_to(coords)\n",
      "change_movement_speed(20)\n",
      "time.sleep(5)\n",
      "coords = detect('left wall')\n",
      "move_to(coords)\n",
      "175 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.5, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(5)\n",
      "move_to(coords)\n",
      "176 th val pred: coords = detect('screw')\n",
      "pick_up(coords)\n",
      "coords = detect('left side', 'sixth largest')\n",
      "place(coords)\n",
      "176 th val ref: coords = detect('sixth largest screw on left')\n",
      "rotate_screwer(coords)\n",
      "177 th val pred: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords = detect('blue bin')\n",
      "place(coords)\n",
      "177 th val ref: coords = detect('left box')\n",
      "pick_up(coords)\n",
      "coords = detect('right blue bin')\n",
      "place(coords)\n",
      "178 th val pred: coords = detect('wall')\n",
      "move_to(coords)\n",
      "178 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.6, coords[2]]\n",
      "move_to(coords)\n",
      "179 th val pred: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('dishwasher')\n",
      "place(coords)\n",
      "179 th val ref: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('dishwasher')\n",
      "place(coords)\n",
      "180 th val pred: coords = detect('floor')\n",
      "coords2 = detect('ceiling')\n",
      "distance = coords[2] - coords2[2]\n",
      "print(f\"The distance between the floor and the ceiling is {distance} meters.\")\n",
      "180 th val ref: coords_floor = detect('floor')\n",
      "coords_ceiling = detect('ceiling')\n",
      "distance = np.sqrt(((coords_floor[0]+coords_ceiling[0])/2)2 + ((coords_floor[1]+coords_ceiling[1])/2)2 + ((coords_floor[2]+coords_ceiling[2])/2)2)\n",
      "print(distance)\n",
      "181 th val pred: coords = detect('front green pen')\n",
      "move_to(coords)\n",
      "181 th val ref: coords = detect('green pen')\n",
      "move_to(coords)\n",
      "182 th val pred: coords = detect('seventh largest screw')\n",
      "pick_up(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "place(coords)\n",
      "182 th val ref: coords = detect('seventh largest screw')\n",
      "rotate_screwer(coords)\n",
      "183 th val pred: coords = detect('small yellow button')\n",
      "move_to(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "pick_up([coords[0], coords[1], coords[2]+0.5])\n",
      "place([coords[0], coords[1], coords[2]])\n",
      "183 th val ref: coords = detect('small yellow button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.2])\n",
      "change_movement_speed(70)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.2])\n",
      "184 th val pred: coords = detect('black marker')\n",
      "move_to(coords)\n",
      "coords = detect('white paper')\n",
      "move_to(coords)\n",
      "184 th val ref: coords = detect('black marker')\n",
      "pick_up(coords)\n",
      "coords = detect('white paper')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "185 th val pred: coords = detect('yellow block')\n",
      "pick_up(coords)\n",
      "coords = detect('green cube')\n",
      "place(coords)\n",
      "185 th val ref: coords = detect('yellow block')\n",
      "pick_up(coords)\n",
      "coords = detect('green cube')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "186 th val pred: coords = detect('red screw')\n",
      "pick_up(coords)\n",
      "coords = detect('hole on the wall')\n",
      "place(coords)\n",
      "186 th val ref: print('Error: can not use screwer when using gripper')\n",
      "187 th val pred: coords = detect('red marker')\n",
      "move_to(coords)\n",
      "coords = detect('white paper')\n",
      "move_to(coords)\n",
      "187 th val ref: coords = detect('red marker')\n",
      "pick_up(coords)\n",
      "coords = detect('white paper')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "188 th val pred: coords = detect('tenth largest screw')\n",
      "pick_up(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "place(coords)\n",
      "188 th val ref: coords = detect('tenth largest screw')\n",
      "rotate_screwer(coords)\n",
      "189 th val pred: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "move_to([coords[0], coords[1] + 0.08, coords[2]])\n",
      "place([coords[0], coords[1] + 0.08, coords[2]])\n",
      "189 th val ref: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 0.08, coords[1], coords[2]]\n",
      "place(coords)\n",
      "190 th val pred: coords = detect('toy car')\n",
      "move_to(coords)\n",
      "190 th val ref: coords = detect('toy car')\n",
      "print(coords)\n",
      "191 th val pred: coords = detect('kettle')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "191 th val ref: coords = detect('kettle')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "192 th val pred: coords = detect('black pencil')\n",
      "move_to(coords)\n",
      "192 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "193 th val pred: coords = detect('eleventh largest screw')\n",
      "pick_up(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "place(coords)\n",
      "193 th val ref: coords = detect('eleventh largest screw')\n",
      "rotate_screwer(coords)\n",
      "194 th val pred: coords = detect('black folder')\n",
      "pick_up(coords)\n",
      "coords = detect('stapler')\n",
      "pick_up(coords)\n",
      "move_to([0, 0, -1])\n",
      "place([0, 0, -1])\n",
      "194 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "195 th val pred: coords = detect('marble shelf')\n",
      "move_to(coords)\n",
      "195 th val ref: coords = detect('marble shelf')\n",
      "move_to(coords)\n",
      "196 th val pred: coords = detect('twelfth largest screw of those on the left side')\n",
      "pick_up(coords)\n",
      "rotate_screwer([0,0,90])\n",
      "place(coords)\n",
      "196 th val ref: coords = detect('twelfth largest screw on left')\n",
      "rotate_screwer(coords)\n"
     ]
    }
   ],
   "source": [
    "for i, (val_pred, val_ref) in enumerate(zip(val_preds, val_refs)):\n",
    "    print(f'{i} th val pred:', val_pred)\n",
    "    print(f'{i} th val ref:', val_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bbce1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = rouge.compute(\n",
    "    predictions=val_preds,\n",
    "    references=val_refs,\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da3ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.6813327036416053),\n",
       " 'rouge2': np.float64(0.5414274110427252),\n",
       " 'rougeL': np.float64(0.6593034186085507),\n",
       " 'rougeLsum': np.float64(0.6760368820424991)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b60c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
