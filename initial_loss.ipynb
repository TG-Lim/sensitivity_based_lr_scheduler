{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b882b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 1. 스크립트 최상단에서 랜덤 시드 고정 함수 정의 및 실행\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# 원하는 시드 값으로 설정 (이 숫자를 바꾸지 않는 한 항상 동일하게 초기화됨)\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# 사용할 모델의 Hugging Face 저장소 이름\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2929301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 토크나이저를 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # 사용 가능한 GPU에 모델을 자동으로 할당합니다.\n",
    "    torch_dtype=torch.bfloat16,  # 모델을 float16으로 로드합니다.\n",
    "    trust_remote_code=True,  # 원격 코드 신뢰 설정\n",
    ")\n",
    "\n",
    "# 해당 모델에 맞는 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left',\n",
    "    use_fast=True)\n",
    "\n",
    "print(\"모델과 토크나이저를 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934c55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None\n",
    "model.generation_config.top_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62dcafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 행렬의 rank (값이 클수록 파라미터 수가 늘어남)\n",
    "    lora_alpha=16,  # LoRA 스케일링 alpha (보통 r의 2배)\n",
    "    # Qwen2 아키텍처의 Attention 관련 레이어를 타겟으로 지정\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.0, # Dropout 비율\n",
    "    bias=\"none\", # bias는 학습하지 않음\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc145f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,490,944 || all params: 1,545,205,248 || trainable%: 0.0965\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 가능한 파라미터 수 출력 (LoRA의 효율성 확인)\n",
    "model.print_trainable_parameters()\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f360224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a8ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # causal language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78871b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa5f12901764b07931b27e0ec83abfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea99c60ebf484ba5b4a48b0f3a329762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_excel(\"data.xlsx\")\n",
    "df[\"system\"] = df[\"system\"].fillna(\"\")  # system 결측값 처리\n",
    "# 3. Train과 Validation으로 데이터 분할\n",
    "train_df = df.sample(frac=0.8, random_state=42)  # 80% Train\n",
    "val_df = df.drop(train_df.index)  # 나머지 20% Validation\n",
    "\n",
    "def convert_to_chat_format(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "    ]\n",
    "\n",
    "train_chat_data = [convert_to_chat_format(row) for _, row in train_df.iterrows()]\n",
    "train_dataset = Dataset.from_dict({\"conversations\": train_chat_data})\n",
    "\n",
    "def apply_chat_template_to_dataset(example):\n",
    "    # conversations 하나씩 처리\n",
    "    conversation = example[\"conversations\"]\n",
    "    \n",
    "    # apply_chat_template 적용\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,  # 아직 토크나이징하지 않고 텍스트만\n",
    "        add_generation_prompt=False  # 학습용이므로 generation prompt 불필요\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "formatted_dataset = train_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 이제 formatted_text들을 실제로 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,  # 필요에 따라 조정\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels는 input_ids와 동일하게 설정\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 토크나이징 적용\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,  # 배치 단위로 처리 (더 효율적)\n",
    "    remove_columns=formatted_dataset.column_names  # 기존 컬럼들 제거 (text 컬럼 등)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a2ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb636a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280e117aa89941d982badbd3eb17a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8ea8e87a0c4035811b295bb28d7b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_metric_format(row):\n",
    "    return {\n",
    "        \"input\": [\n",
    "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]}\n",
    "        ],\n",
    "        \"target\": row[\"response\"]  # 정답 assistant 응답\n",
    "    }\n",
    "\n",
    "val_chat_data = [convert_to_chat_format(row) for _, row in val_df.iterrows()]\n",
    "val_dataset = Dataset.from_dict({\"conversations\": val_chat_data})\n",
    "\n",
    "val_metric_data = [create_metric_format(row) for _, row in val_df.iterrows()]\n",
    "val_metric_dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in val_metric_data],\n",
    "    \"target\": [item[\"target\"] for item in val_metric_data]\n",
    "})\n",
    "\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "val_formatted_dataset = val_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "# 토크나이징 적용\n",
    "val_tokenized_dataset = val_formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_formatted_dataset.column_names\n",
    ")\n",
    "\n",
    "# Validation DataLoader 생성\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,  # validation은 섞지 않음\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da05cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "Total trainable parameters: 168\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        cnt += 1\n",
    "        print(f\"{name} - {param.shape}\")\n",
    "print(f\"Total trainable parameters: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e44355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.731632598160487\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "initial_train_loss = 0.0\n",
    "for batch in train_loader:\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    initial_train_loss += loss.item()\n",
    "initial_train_loss /= len(train_loader)\n",
    "print(initial_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94d0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.719968952313818\n"
     ]
    }
   ],
   "source": [
    "initial_val_loss = 0.0\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    initial_val_loss += loss.item()\n",
    "initial_val_loss /= len(val_loader)\n",
    "print(initial_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e450d0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./initial_loss.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump({\n",
    "    'train': initial_train_loss,\n",
    "    'val': initial_val_loss\n",
    "}, './initial_loss.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294d786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
