{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45857777",
   "metadata": {},
   "source": [
    "### Model 준비과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff824c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 1. 스크립트 최상단에서 랜덤 시드 고정 함수 정의 및 실행\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# 원하는 시드 값으로 설정 (이 숫자를 바꾸지 않는 한 항상 동일하게 초기화됨)\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# 사용할 모델의 Hugging Face 저장소 이름\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc905f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 토크나이저를 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # 사용 가능한 GPU에 모델을 자동으로 할당합니다.\n",
    "    torch_dtype=torch.bfloat16,  # 모델을 float16으로 로드합니다.\n",
    "    trust_remote_code=True,  # 원격 코드 신뢰 설정\n",
    ")\n",
    "\n",
    "# 해당 모델에 맞는 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left',\n",
    "    use_fast=True)\n",
    "\n",
    "print(\"모델과 토크나이저를 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_p = None\n",
    "model.generation_config.top_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7b9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 행렬의 rank (값이 클수록 파라미터 수가 늘어남)\n",
    "    lora_alpha=16,  # LoRA 스케일링 alpha (보통 r의 2배)\n",
    "    # Qwen2 아키텍처의 Attention 관련 레이어를 타겟으로 지정\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.0, # Dropout 비율\n",
    "    bias=\"none\", # bias는 학습하지 않음\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02103835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,490,944 || all params: 1,545,205,248 || trainable%: 0.0965\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 가능한 파라미터 수 출력 (LoRA의 효율성 확인)\n",
    "model.print_trainable_parameters()\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44f4a2",
   "metadata": {},
   "source": [
    "### Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b194f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f267bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # causal language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba4eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f8fbd59a3c436f88b47c15924891e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16e16fb0a8742fc918c2ef0a2ec56ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_excel(\"data.xlsx\")\n",
    "df[\"system\"] = df[\"system\"].fillna(\"\")  # system 결측값 처리\n",
    "# 3. Train과 Validation으로 데이터 분할\n",
    "train_df = df.sample(frac=0.8, random_state=42)  # 80% Train\n",
    "val_df = df.drop(train_df.index)  # 나머지 20% Validation\n",
    "\n",
    "def convert_to_chat_format(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "    ]\n",
    "\n",
    "train_chat_data = [convert_to_chat_format(row) for _, row in train_df.iterrows()]\n",
    "train_dataset = Dataset.from_dict({\"conversations\": train_chat_data})\n",
    "\n",
    "def apply_chat_template_to_dataset(example):\n",
    "    # conversations 하나씩 처리\n",
    "    conversation = example[\"conversations\"]\n",
    "    \n",
    "    # apply_chat_template 적용\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,  # 아직 토크나이징하지 않고 텍스트만\n",
    "        add_generation_prompt=False  # 학습용이므로 generation prompt 불필요\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "formatted_dataset = train_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 이제 formatted_text들을 실제로 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,  # 필요에 따라 조정\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels는 input_ids와 동일하게 설정\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 토크나이징 적용\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,  # 배치 단위로 처리 (더 효율적)\n",
    "    remove_columns=formatted_dataset.column_names  # 기존 컬럼들 제거 (text 컬럼 등)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d01d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1576f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c33ecfb58b84b19be3cfd3d52f47db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabaf694c11849569bd9c3983d3ac434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_metric_format(row):\n",
    "    return {\n",
    "        \"input\": [\n",
    "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]}\n",
    "        ],\n",
    "        \"target\": row[\"response\"]  # 정답 assistant 응답\n",
    "    }\n",
    "\n",
    "val_chat_data = [convert_to_chat_format(row) for _, row in val_df.iterrows()]\n",
    "val_dataset = Dataset.from_dict({\"conversations\": val_chat_data})\n",
    "\n",
    "val_metric_data = [create_metric_format(row) for _, row in val_df.iterrows()]\n",
    "val_metric_dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in val_metric_data],\n",
    "    \"target\": [item[\"target\"] for item in val_metric_data]\n",
    "})\n",
    "\n",
    "\n",
    "# 채팅 템플릿 적용\n",
    "val_formatted_dataset = val_dataset.map(apply_chat_template_to_dataset)\n",
    "\n",
    "# 토크나이징 적용\n",
    "val_tokenized_dataset = val_formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_formatted_dataset.column_names\n",
    ")\n",
    "\n",
    "# Validation DataLoader 생성\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,  # validation은 섞지 않음\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49c7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfeb00",
   "metadata": {},
   "source": [
    "### 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15a1f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.Size([1536, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.Size([8, 1536])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.Size([256, 8])\n",
      "Total trainable parameters: 168\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        cnt += 1\n",
    "        print(f\"{name} - {param.shape}\")\n",
    "print(f\"Total trainable parameters: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc43f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12f61ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR = 1e-5\n",
    "MAX_STEPS = 500\n",
    "EVAL_STEPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974a38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "param_groups = [{\"params\": [p]} for p in trainable_params]\n",
    "optimizer = AdamW(param_groups, lr=BASE_LR)\n",
    "scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afac953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Training Progress: 100%|██████████| 500/500 [16:32<00:00,  1.99s/it, train_loss=1.9682, val_loss=1.9139] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "history = {\n",
    "    \"steps\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "model.train()\n",
    "progress_bar = tqdm(total=MAX_STEPS, desc=\"Overall Training Progress\")\n",
    "\n",
    "while global_step < MAX_STEPS:\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if global_step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # 1. forward\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 2. optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # 3. 정해진 스텝마다 평가\n",
    "        if global_step % EVAL_STEPS == 0:\n",
    "            avg_train_loss = loss.item()\n",
    "            history[\"steps\"].append(global_step)\n",
    "            history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "            # --- Validation Loss calculation ---\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_batch = {k: v.to(model.device) for k, v in val_batch.items()}\n",
    "                    val_outputs = model(**val_batch)\n",
    "                    val_loss += val_outputs.loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                train_loss=f\"{avg_train_loss:.4f}\",\n",
    "                val_loss=f\"{avg_val_loss:.4f}\",\n",
    "            )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b517248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp/training_history_cosine.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(history, \"temp/training_history_cosine.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fba106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub('kyu5787/robot_task_planning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e45cbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cf249f8140>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67607723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acd0195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [04:48<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "val_preds, val_refs = [], []\n",
    "\n",
    "for item in tqdm(val_metric_dataset):\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        item['input'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    if \"assistant\\n\" in generated_text:\n",
    "        pred = generated_text.split(\"assistant\\n\", 1)[1].strip()\n",
    "    else:\n",
    "        pred = generated_text.strip()  # fallback\n",
    "    \n",
    "    val_preds.append(pred)\n",
    "    val_refs.append(item['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aed1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th val pred: [0, 0, 0] = detect('keyboard')\n",
      "0 th val ref: coords = detect('keyboard')\n",
      "move_to(coords)\n",
      "1 th val pred: [0.0, 0.0, 1.0], [0.0, 0.0, 2.0]\n",
      "1 th val ref: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 5, coords[1], coords[2]]\n",
      "place(coords)\n",
      "2 th val pred: [0.5, 0.0, 0.0] = detect('object')\n",
      "move_to([0.0, 0.0, 0.5])\n",
      "change_movement_speed(40)\n",
      "2 th val ref: change_movement_speed(40)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2]+0.5]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "3 th val pred: change_movement_speed(45)\n",
      "3 th val ref: change_movement_speed(45)\n",
      "4 th val pred: [0.5, 1.0, 2.0] = detect('box')\n",
      "rotate_screwer([0.5, 1.0, 2.0])\n",
      "move_to([0.5, 1.0, 3.0])\n",
      "pick_up([0.5, 1.0, 3.0])\n",
      "place([0.5, 1.0, 4.0])\n",
      "4 th val ref: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "5 th val pred: [0, 0, 1] = detect('table')\n",
      "[0, 0, 2] = detect('wall')\n",
      "distance = abs(0 - 2)\n",
      "print(distance)\n",
      "5 th val ref: coords_table = detect('table')\n",
      "coords_wall = detect('wall')\n",
      "distance = np.sqrt(((coords_table[0]+coords_wall[0])/2)2 + ((coords_table[1]+coords_wall[1])/2)2 + ((coords_table[2]+coords_wall[2])/2)2)\n",
      "print(distance)\n",
      "6 th val pred: [move_to([-0.1,0,0]),change_movement_speed(70),wait(2),move_to([0,0,0])]\n",
      "6 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]+0.25, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(2)\n",
      "move_to(coords)\n",
      "7 th val pred: [0.5, 0.5, 0.5] = detect('yellow screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "7 th val ref: print('Error: can not use screwer when using gripper')\n",
      "8 th val pred: [move_to([0,0,1]), detect(\"green_block\"), pick_up([0,0,1]), move_to([0,0,-2]), detect(\"yellow_block\"), place([0,0,-2]), move_to([0,0,1]), detect(\"blue_block\"), pick_up([0,0,1]), move_to([0,0,-3]), detect(\"yellow_block\"), place([0,0,-3])]\n",
      "8 th val ref: coords_g = detect('green block')\n",
      "coords_y = detect('yellow block')\n",
      "coords_b = detect('blue block')\n",
      "coords_y = [coords_y[0], coords_y[1], coords_y[2]+0.2]\n",
      "pick_up(coords_g)\n",
      "place(coords_y)\n",
      "pick_up(coords_y)\n",
      "place(coords_g)\n",
      "pick_up(coords_b)\n",
      "place(coords_y)\n",
      "9 th val pred: [0.5, 0.5, 0.5] = detect('white ruler')\n",
      "[1.0, 0.5, 0.5] = detect('small battery')\n",
      "\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "\n",
      "place([1.0, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "9 th val ref: coords_r = detect('white ruler')\n",
      "coords_b = detect('small battery')\n",
      "coords_b2 = [coords_b[0], coords_b[1]+0.3, coords_b[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_b2)\n",
      "pick_up(coords_b)\n",
      "place(coords_r)\n",
      "pick_up(coords_b2)\n",
      "place(coords_b)\n",
      "10 th val pred: [0.2, 0.0, 0.0], [-0.0, -0.005, 0.0], [0.0, 0.0, 1.0]\n",
      "10 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.03, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.005]\n",
      "move_to(coords)\n",
      "coords = [coords[0]+1, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "11 th val pred: [0.5, 1.0, 2.0] = detect('yellow block')\n",
      "11 th val ref: coords = detect('yellow block')\n",
      "move_to(coords)\n",
      "12 th val pred: [0.5, 1.2, 0.8], [0.7, 1.3, 0.9]\n",
      "12 th val ref: coords = detect('blue bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('green shelf')\n",
      "place(coords)\n",
      "13 th val pred: [0.5, 0.5, 1]\n",
      "13 th val ref: coords = detect('printer')\n",
      "move_to(coords)\n",
      "14 th val pred: [0.5, 1.2, 0.8]\n",
      "14 th val ref: coords_door = detect('door')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_door[0]+coords_window[0])/2)2 + ((coords_door[1]+coords_window[1])/2)2 + ((coords_door[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "15 th val pred: [0.5, 0.5, 0.5], [1.2, 0.7, 0.3]\n",
      "15 th val ref: coords = detect('orange pen')\n",
      "pick_up(coords)\n",
      "coords = detect('blue book')\n",
      "coords = [coords[0], coords[1]-0.2, coords[2]]\n",
      "place(coords)\n",
      "16 th val pred: [0.5, 0.5, 0.5] = detect('screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "16 th val ref: coords = detect('leftmost screw')\n",
      "rotate_screwer(coords)\n",
      "17 th val pred: [move_to([0,0,-5]),rotate_screwer([0,0,90]),move_to([-2,0,0])]\n",
      "17 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.05, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]+0.01]\n",
      "move_to(coords)\n",
      "coords = [coords[0]+2, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "18 th val pred: [0.5, 0.5, 0.5] = detect('red block')\n",
      "move_to(detect('red block'))\n",
      "18 th val ref: coords = detect('red block')\n",
      "move_to(coords)\n",
      "19 th val pred: [0.5, 1.0, 0.5] = detect('green block')\n",
      "19 th val ref: coords = detect('green block')\n",
      "move_to(coords)\n",
      "20 th val pred: [0.5, 1, 2]\n",
      "20 th val ref: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "21 th val pred: [0.8, 0.8], [20, 0, 15]\n",
      "21 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]+0.15]\n",
      "place(coords)\n",
      "22 th val pred: [0.5, 1, 0], [0.5, 1, 0]\n",
      "22 th val ref: coords = detect('chair')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "23 th val pred: [0.5, 1, 2]\n",
      "23 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('charging station')\n",
      "place(coords)\n",
      "24 th val pred: [move_to([0,0,1]), detect('screws'), rotate_screwer([0,0,1]), pick_up([0,0,1]), move_to([0,0,-1]), place([0,0,-1])]\n",
      "24 th val ref: coords_g = detect('green screw')\n",
      "coords_r = detect('red screw')\n",
      "coords_b = detect('blue screw')\n",
      "coords_nextto_g = [coords_g[0] - 0.1, coords_g[1], coords_g[2]]\n",
      "coords_nextto_r = [coords_g[0] - 0.2, coords_g[1], coords_g[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_nextto_g)\n",
      "pick_up(coords_b)\n",
      "place(coords_nextto_r)\n",
      "25 th val pred: [0.5, -1, 0]\n",
      "25 th val ref: coords = detect('yellow box')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1]-1, coords[2]]\n",
      "place(coords)\n",
      "26 th val pred: [0.5, 0.5, 0.5] = detect('blue mat')\n",
      "26 th val ref: coords = detect('blue mat')\n",
      "move_to(coords)\n",
      "27 th val pred: [move_to([-10,0,-30]),change_movement_speed(50)]\n",
      "27 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.10, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.3]\n",
      "move_to(coords)\n",
      "28 th val pred: [0.5, 0.5, 0.5] = detect('red bin')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "28 th val ref: coords = detect('red bin')\n",
      "move_to(coords)\n",
      "29 th val pred: [0.0, -0.05, 0.0]\n",
      "29 th val ref: coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "coords = [coords[0]+0.05, coords[1], coords[2]]\n",
      "place(coords)\n",
      "30 th val pred: [0, 0, 1]\n",
      "30 th val ref: coords = detect('block')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "31 th val pred: [0, 1, 0]\n",
      "31 th val ref: coords = detect('block')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "32 th val pred: [0.5, 1, 2]\n",
      "32 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('reading shelf')\n",
      "place(coords)\n",
      "33 th val pred: [desk_x, desk_y, desk_z] = detect('desk')\n",
      "[lamp_x, lamp_y, lamp_z] = detect('lamp')\n",
      "\n",
      "distance = calculate_distance(desk_x, desk_y, desk_z, lamp_x, lamp_y, lamp_z)\n",
      "\n",
      "print(f\"The distance between the desk and the lamp is {distance} units.\")\n",
      "33 th val ref: coords_desk = detect('desk')\n",
      "coords_lamp = detect('lamp')\n",
      "distance = np.sqrt(((coords_desk[0]+coords_lamp[0])/2)2 + ((coords_desk[1]+coords_lamp[1])/2)2 + ((coords_desk[2]+coords_lamp[2])/2)2)\n",
      "print(distance)\n",
      "34 th val pred: pick_up([0.5, 1, 2])\n",
      "move_to([0.5, 1, 3])\n",
      "detect(\"books\")\n",
      "rotate_screwer([0.5, 1, 4])\n",
      "place([0.5, 1, 5])\n",
      "change_movement_speed(70)\n",
      "pick_up([0.5, 1, 6])\n",
      "move_to([0.5, 1, 7])\n",
      "detect(\"color\")\n",
      "rotate_screwer([0.5, 1, 8])\n",
      "place([0.5, 1, 9])\n",
      "change_movement_speed(70)\n",
      "34 th val ref: coords_r = detect('red book')\n",
      "coords_g = detect('green book')\n",
      "coords_b = detect('blue book')\n",
      "coords_nextto_r = [coords_r[0] - 0.1, coords_r[1], coords_r[2]]\n",
      "coords_nextto_g = [coords_r[0] - 0.2, coords_r[1], coords_r[2]]\n",
      "pick_up(coords_g)\n",
      "place(coords_nextto_r)\n",
      "pick_up(coords_b)\n",
      "place(coords_nextto_g)\n",
      "35 th val pred: [0.5, 0.5, 0.5] = detect('yellow ruler')\n",
      "[0.7, 0.3, 0.2] = detect('blue notebook')\n",
      "rotate_screwer([0.6, 0.4, 0.3])\n",
      "move_to([0.8, 0.6, 0.4])\n",
      "change_movement_speed(100)\n",
      "pick_up([0.8, 0.6, 0.4])\n",
      "place([0.9, 0.7, 0.5])\n",
      "35 th val ref: coords = detect('yellow ruler')\n",
      "pick_up(coords)\n",
      "coords = detect('blue notebook')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "36 th val pred: [0.5, 1, 0.5] = detect('laptop')\n",
      "move_to([0.5, 1, 0.5])\n",
      "rotate_screwer([0.5, 1, 0.5])\n",
      "pick_up([0.5, 1, 0.5])\n",
      "place([0.5, 1, 0.5])\n",
      "36 th val ref: coords = detect('laptop')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "37 th val pred: [0.5,0,0],place([0.5,0,0])\n",
      "37 th val ref: coords = detect('ruler')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "38 th val pred: [red, blue, green]\n",
      "38 th val ref: coords_y = detect('yellow block')\n",
      "coords_g = detect('green block')\n",
      "coords_b = detect('blue block')\n",
      "coords_y = [coords_y[0], coords_y[1], coords_y[2]+0.2]\n",
      "pick_up(coords_y)\n",
      "place(coords_g)\n",
      "pick_up(coords_g)\n",
      "place(coords_y)\n",
      "pick_up(coords_b)\n",
      "place(coords_g)\n",
      "39 th val pred: [0.5, 0.5, 0.5] = detect('green pencil')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "39 th val ref: coords = detect('rear green pencil')\n",
      "move_to(coords)\n",
      "40 th val pred: [0.5, 0.5, 1], rotate_screwer([0.5, 0.5, 1]), move_to([0.5, 0.5, 2]), detect(\"grey bin\"), place([0.5, 0.5, 3])\n",
      "40 th val ref: coords = detect('grey bin')\n",
      "move_to(coords)\n",
      "41 th val pred: [0.5, 0.5, 0.5] = detect('red pencil')\n",
      "[1.0, 1.0, 1.0] = detect('blue battery')\n",
      "\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "\n",
      "place([1.0, 1.0, 1.0])\n",
      "\n",
      "change_movement_speed(100)\n",
      "\n",
      "move_to([1.0, 1\n",
      "41 th val ref: coords_r = detect('red pencil')\n",
      "coords_b = detect('blue battery')\n",
      "coords_b2 = [coords_b[0], coords_b[1]+0.3, coords_b[2]]\n",
      "pick_up(coords_r)\n",
      "place(coords_b2)\n",
      "pick_up(coords_b)\n",
      "place(coords_r)\n",
      "pick_up(coords_b2)\n",
      "place(coords_b)\n",
      "42 th val pred: [0.1,0.2,0.3]\n",
      "42 th val ref: coords = detect('smallest pencil')\n",
      "pick_up(coords)\n",
      "move_to([coords[0],coords[1],coords[2]+0.5])\n",
      "43 th val pred: [0.5, 1.0, 0.0], [0.75, 1.0, 0.0]\n",
      "43 th val ref: coords = detect('red block')\n",
      "pick_up(coords)\n",
      "coords = detect('blue block')\n",
      "coords = [coords[0], coords[1], coords[2]-0.2]\n",
      "place(coords)\n",
      "44 th val pred: [move_to([-0.02,0,0]),change_movement_speed(100),move_to([0,0.5,0])]\n",
      "44 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.02, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "coords = [coords[0], coords[1], coords[2]+0.5]\n",
      "move_to(coords)\n",
      "45 th val pred: [0.5, 0.5, 0.5], [1.0, 0.5, 0.5]\n",
      "45 th val ref: coords = detect('hammer')\n",
      "pick_up(coords)\n",
      "coords = [coords[0]-0.2, coords[1], coords[2]]\n",
      "place(coords)\n",
      "46 th val pred: [0.5, 0.5, 0.5] = detect('chair')\n",
      "[0.7, 0.8, 0.9] = detect('table')\n",
      "move_to([0.6, 0.7, 0.8])\n",
      "rotate_screwer([0.6, 0.7, 0.8])\n",
      "pick_up([0.6, 0.7, 0.8])\n",
      "place([0.6, 0.7, 0.8])\n",
      "46 th val ref: coords = detect('green bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('red shelf')\n",
      "place(coords)\n",
      "47 th val pred: [0.5, 1, 0.5] = detect('hammer')\n",
      "rotate_screwer([0.5, 1, 0.5])\n",
      "move_to([0.5, 1, 0.5])\n",
      "change_movement_speed(80)\n",
      "pick_up([0.5, 1, 0.5])\n",
      "place([0.5, 1, 0.5])\n",
      "47 th val ref: coords = detect('chair')\n",
      "move_to(coords)\n",
      "coords_table = detect('table')\n",
      "coords = [coords_table[0]+0.1, coords_table[1], coords_table[2]]\n",
      "place(coords)\n",
      "48 th val pred: [move_to([0,0,1]), detect('screws'), rotate_screwer([0,0,1]), pick_up([0,0,1]), move_to([0,0,-1]), place([0,0,-1])]\n",
      "48 th val ref: coords_door = detect('door')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_door[0]+coords_window[0])/2)2 + ((coords_door[1]+coords_window[1])/2)2 + ((coords_door[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "49 th val pred: [0.5, 1, 2]\n",
      "49 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.20, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "coords2 = [coords[0]+0.15, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "50 th val pred: [0.5, 0.5, 0.5] = detect('middle screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "50 th val ref: coords = detect('screw closest to door')\n",
      "rotate_screwer(coords)\n",
      "51 th val pred: [0.5, 0.5, 0.5] = detect('screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "51 th val ref: coords = detect('middle screw on right')\n",
      "rotate_screwer(coords)\n",
      "52 th val pred: [0.5, 0.5, 1]\n",
      "52 th val ref: coords = detect('first screw on left')\n",
      "rotate_screwer(coords)\n",
      "53 th val pred: [0.5, 1, 0.5] = detect('lamp')\n",
      "[0.5, 1, 0.5] = detect('desk')\n",
      "move_to([0.5, 1, 0.5])\n",
      "rotate_screwer([0.5, 1, 0.5])\n",
      "pick_up([0.5, 1, 0.5])\n",
      "place([0.5, 1, 0.5])\n",
      "53 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('nightstand')\n",
      "place(coords)\n",
      "54 th val pred: [0.5, 0.0, 0.0] = detect('black shirt')\n",
      "rotate_screwer([0.5, 0.0, 0.0])\n",
      "move_to([0.6, 0.0, 0.0])\n",
      "54 th val ref: coords = detect('lamp')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "55 th val pred: [0.5, 1.0, 2.0] = detect('box')\n",
      "rotate_screwer([0.5, 1.0, 2.0])\n",
      "move_to([0.5, 1.0, 3.0])\n",
      "pick_up([0.5, 1.0, 3.0])\n",
      "place([0.5, 1.0, 4.0])\n",
      "55 th val ref: coords = detect('pillow')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 0.8, coords[1], coords[2] + 0.3]\n",
      "place(coords)\n",
      "56 th val pred: [0.5, 1.2, 0.8]\n",
      "56 th val ref: coords = detect('box')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "57 th val pred: [0.5, 0.5, 0.5] = detect('green umbrella')\n",
      "move_to(detect('green umbrella'))\n",
      "57 th val ref: coords = detect('green umbrella')\n",
      "move_to(coords)\n",
      "58 th val pred: [move_to([-40,0,0]),change_movement_speed(50),wait(2),move_to([0,0,0])]\n",
      "58 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]+0.4, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(2)\n",
      "move_to(coords)\n",
      "59 th val pred: [0.5, 1.2, 0.8], [0.7, 1.3, 0.9]\n",
      "59 th val ref: coords = detect('blue bottle')\n",
      "pick_up(coords)\n",
      "coords = detect('green shelf')\n",
      "place(coords)\n",
      "60 th val pred: [0.5, 1, 0.5] = detect('book')\n",
      "move_to([0.5, 1, 0.5])\n",
      "rotate_screwer([0.5, 1, 0.5])\n",
      "pick_up([0.5, 1, 0.5])\n",
      "place([0.5, 1, 0.5])\n",
      "60 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('desk')\n",
      "place(coords)\n",
      "61 th val pred: [0.5, 1.2, 0.8], [0.7, 1.3, 0.9]\n",
      "61 th val ref: coords = detect('blue box')\n",
      "coords_shelf = detect('green shelf')\n",
      "place(coords_shelf)\n",
      "62 th val pred: [0.5, 0.5, 0.5] = detect('black bottle')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.7, 0.5, 0.5])\n",
      "change_movement_speed(100)\n",
      "pick_up([0.7, 0.5, 0.5])\n",
      "place([0.8, 0.5, 0.5])\n",
      "62 th val ref: coords = detect('black bottle')\n",
      "coords_box = detect('red box')\n",
      "coords = [coords_box[0]-0.2, coords_box[1], coords_box[2]]\n",
      "place(coords)\n",
      "63 th val pred: [0.5, 1, 2]\n",
      "63 th val ref: coords = detect('large hammer')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('shelf')\n",
      "place(coords_shelf)\n",
      "64 th val pred: [0.5, 0.5, 0.5] = detect('screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "64 th val ref: coords = detect('screw closest to wall')\n",
      "rotate_screwer(coords)\n",
      "65 th val pred: [0.5,0.5,0.5],place([0.5,0.5,0.5]))\n",
      "65 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "66 th val pred: [0.5, 1.0, 0.0] = detect('station_2')\n",
      "66 th val ref: coords = detect('robotic arm')\n",
      "pick_up(coords)\n",
      "coords = detect('station 2')\n",
      "place(coords)\n",
      "67 th val pred: [0.5, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.\n",
      "67 th val ref: coords = detect('right pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red bin')\n",
      "place(coords)\n",
      "68 th val pred: [0, -1, 0] = detect('floor')\n",
      "move_to([0, -1, 0], 50)\n",
      "68 th val ref: change_movement_speed(50)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2] - 1]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "69 th val pred: [0.5, 0.5, 1]\n",
      "69 th val ref: coords = detect('printer')\n",
      "move_to(coords)\n",
      "70 th val pred: [0.5, 0.5, 0.5], [1.0, 0.0, 0.0]\n",
      "70 th val ref: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('red block')\n",
      "coords = [coords[0], coords[1] + 0.2, coords[2]]\n",
      "place(coords)\n",
      "71 th val pred: [0.5, 1, 2]\n",
      "71 th val ref: coords = detect('second largest screw')\n",
      "rotate_screwer(coords)\n",
      "72 th val pred: [0.5, 1, 2]\n",
      "72 th val ref: coords = detect('third smallest screw on right')\n",
      "rotate_screwer(coords)\n",
      "73 th val pred: [0.5, 0.5, 1]\n",
      "73 th val ref: coords = detect('door')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "74 th val pred: [0.5, 0.5, 0.5], [0.5, 0.5, 0.7]\n",
      "74 th val ref: coords = detect('cup')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "75 th val pred: [0,0,0] = detect('station_1')\n",
      "75 th val ref: coords = detect('robotic arm')\n",
      "pick_up(coords)\n",
      "coords = detect('station 1')\n",
      "place(coords)\n",
      "76 th val pred: [0.5, 1, 2]\n",
      "76 th val ref: coords = detect('nail')\n",
      "coords_w = detect('wrench')\n",
      "coords_h = detect('hammer')\n",
      "coords = [(coords_w[0] + coords_h[0]) / 2, (coords_w[1] + coords_h[1]) / 2, (coords_w[2] + coords_h[2]) / 2]\n",
      "place(coords)\n",
      "77 th val pred: [0.5, 1, 2]\n",
      "77 th val ref: coords = detect('wrench')\n",
      "print(coords)\n",
      "78 th val pred: [15, 0, 0]\n",
      "78 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] - 0.02, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "79 th val pred: [0.5, -1, 0]\n",
      "79 th val ref: coords = detect('pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 2, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "80 th val pred: [0.5, 0.5, 0.5] = detect(\"green pen\")\n",
      "move_to(detect(\"green pen\"))\n",
      "80 th val ref: coords = detect('green pen')\n",
      "move_to(coords)\n",
      "81 th val pred: [0.5, 0.5, 0.5] = detect('red phone')\n",
      "[0.5, 0.5, 0.5] = detect('ruler')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "81 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "82 th val pred: [0.5, 0.5, 0.5] = detect('monitor')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5 + 3, 0.5 + 2, 0.5])\n",
      "82 th val ref: coords = detect('monitor')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 3, coords[1] + 2, coords[2]]\n",
      "place(coords)\n",
      "83 th val pred: [0.5, 1.0, 2.0]\n",
      "83 th val ref: coords = detect('smallest bolt')\n",
      "rotate_screwer(coords)\n",
      "84 th val pred: [0, 1, 0]\n",
      "84 th val ref: coords = detect('shelf')\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "85 th val pred: [0.5, 1.0, 0.0], [0.5, 1.0, 0.0]\n",
      "85 th val ref: coords = detect('right tablet')\n",
      "pick_up(coords)\n",
      "coords = detect('white bin')\n",
      "place(coords)\n",
      "86 th val pred: [0.5, 1, 0.5] = detect('stapler')\n",
      "place([0.7, 0.8, 0.9])\n",
      "86 th val ref: coords = detect('stapler on shelf')\n",
      "pick_up(coords)\n",
      "coords = detect('drawer')\n",
      "place(coords)\n",
      "87 th val pred: [0.1, -0.3, 0]\n",
      "87 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] + 0.03, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "88 th val pred: [move_to([0,0,0]),change_movement_speed(50),rotate_screwer([0,0,0]),move_to([10,0,0])]\n",
      "88 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1] + 0.1, coords[2]]\n",
      "move_to(coords)\n",
      "89 th val pred: [0.5, 1.0, 2.0]\n",
      "89 th val ref: coords = detect('projector')\n",
      "move_to(coords)\n",
      "90 th val pred: [0.5, 1, 0]\n",
      "90 th val ref: coords = detect('glass')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "91 th val pred: [0.5, 0.5, 0.5] = detect(blue_pen)\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "91 th val ref: coords = detect('rear blue pen')\n",
      "move_to(coords)\n",
      "92 th val pred: [0.5, -1, 0]\n",
      "92 th val ref: coords = detect('cup')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 2, coords[1], coords[2]]\n",
      "place(coords)\n",
      "93 th val pred: [0.5, 0.5, 0.5] = detect('red screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "93 th val ref: coords = detect('right red screw')\n",
      "rotate_screwer(coords)\n",
      "94 th val pred: [0.5, 1.2, 0.8], [0.7, 1.3, 0.9]\n",
      "94 th val ref: coords = detect('screwdriver')\n",
      "coords_b = detect('bottle')\n",
      "coords_s = detect('box')\n",
      "coords = [(coords_b[0]+coords_s[0])/2, (coords_b[1]+coords_s[1])/2, (coords_b[2]+coords_s[2])/2]\n",
      "place(coords)\n",
      "95 th val pred: [0.5, 1.2, 0.8]\n",
      "95 th val ref: coords = detect('hammer')\n",
      "print(coords)\n",
      "96 th val pred: [0.5, 0.5, 0.5] = detect('can')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "96 th val ref: coords = detect('can')\n",
      "pick_up(coords)\n",
      "coords = detect('recycling bin')\n",
      "place(coords)\n",
      "97 th val pred: [0.5, 0.5, 0.5] = detect('yellow pen')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "97 th val ref: coords = detect('yellow pen')\n",
      "move_to(coords)\n",
      "98 th val pred: [0.5, 0.5, 0.5] = detect('dark drawer')\n",
      "move_to(detect('dark drawer'))\n",
      "98 th val ref: coords = detect('dark drawer')\n",
      "move_to(coords)\n",
      "99 th val pred: [move_to([150,0,0]),change_movement_speed(70),rotate_screwer([150,0,0]),move_to([-150,0,0]),change_movement_speed(70)]\n",
      "99 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.3, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "move_to(coords)\n",
      "100 th val pred: [0, 0, 0]\n",
      "100 th val ref: coords = detect('small green button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.3])\n",
      "change_movement_speed(50)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.3])\n",
      "101 th val pred: [0.5, 1.0, 0.0]\n",
      "101 th val ref: coords = detect('blue marker')\n",
      "pick_up(coords)\n",
      "102 th val pred: [0.5, 0.5, 1] = detect('front door')\n",
      "move_to([0.5, 0.5, 1])\n",
      "rotate_screwer([0.5, 0.5, 1])\n",
      "pick_up([0.5, 0.5, 1])\n",
      "place([0.5, 0.5, 1])\n",
      "102 th val ref: coords = detect('front door')\n",
      "move_to(coords)\n",
      "103 th val pred: [0.5, 0.2, 0.1] = detect('bolt')\n",
      "rotate_screwer([0.5, 0.2, 0.1])\n",
      "move_to([0.5, 0.2, 0.1])\n",
      "pick_up([0.5, 0.2, 0.1])\n",
      "place([0.5, 0.2, 0.1])\n",
      "103 th val ref: coords = detect('bolt near bottom')\n",
      "rotate_screwer(coords)\n",
      "104 th val pred: [0.5, 1.2, 0.8] = detect(\"whiteboard\")\n",
      "104 th val ref: coords = detect('whiteboard')\n",
      "move_to(coords)\n",
      "105 th val pred: [0.5, 1.2, 0.8]\n",
      "105 th val ref: coords_c = detect('chair')\n",
      "coords_d = detect('door')\n",
      "distance = np.sqrt(((coords_c[0] + coords_d[0]) / 2)  2 + ((coords_c[1] + coords_d[1]) / 2)  2 + ((coords_c[2] + coords_d[2]) / 2)  2)\n",
      "print(distance)\n",
      "106 th val pred: [move_to([0,0,0]),change_movement_speed(50),wait(3),move_to([0,0,0])]\n",
      "106 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0] - 0.1, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(3)\n",
      "move_to(coords)\n",
      "107 th val pred: [0,0,0] = detect('wall')\n",
      "move_to(detect('wall'))\n",
      "107 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.15, coords[2]]\n",
      "move_to(coords)\n",
      "108 th val pred: [0.5, 0.5, 0.5], rotate_screwer([0.5, 0.5, 0.5]), move_to([0.5, 0.5, 0.5]), detect(\"red pen\"), move_to([0.7, 0.5, 0.5]), place(\"blue block\")\n",
      "108 th val ref: coords = detect('red pen')\n",
      "pick_up(coords)\n",
      "coords = detect('blue block')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "109 th val pred: [move_to([-0.5, 0, 1]), rotate_screwer([-0.5, 0, 1]), pick_up([0.5, -0.5, 1]), place([0.5, -0.5, 2])]\n",
      "109 th val ref: coords = detect('largest screw on left')\n",
      "rotate_screwer(coords)\n",
      "110 th val pred: [0.5, 0.5, 0.5] = detect('blue screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "110 th val ref: print('Error: can not use screwer when using gripper')\n",
      "111 th val pred: [0.5, 0.5, 0.5] = detect('small blue button')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "111 th val ref: coords = detect('small blue button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.1])\n",
      "change_movement_speed(50)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.1])\n",
      "112 th val pred: [0.5, 1.2, 0.8]\n",
      "112 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords_shelf = detect('top shelf')\n",
      "place(coords_shelf)\n",
      "113 th val pred: [0.5, 0.5, 0.5] = detect('screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "move_to([0.7, 0.5, 0.5])\n",
      "place([0.7, 0.5, 0.5])\n",
      "change_movement_speed(100)\n",
      "rotate_screwer([0.7, 0.5, 0.5])\n",
      "pick_up([0.7, 0.5\n",
      "113 th val ref: coords = detect('right screw')\n",
      "rotate_screwer(coords)\n",
      "114 th val pred: [move_to([0,0,1]), detect('screw'), rotate_screwer([0,0,1]), pick_up([0,0,1]), move_to([0,0,2]), place([0,0,2]), rotate_screwer([0,0,2]), pick_up([0,0,2]), move_to([0,0,3]), place([0,0,3])]\n",
      "114 th val ref: coords_s = detect('small screw')\n",
      "coords_m = detect('medium screw')\n",
      "coords_l = detect('large screw')\n",
      "coords_nextto_s = [coords_s[0] - 0.1, coords_s[1], coords_s[2]]\n",
      "coords_nextto_m = [coords_s[0] - 0.2, coords_s[1], coords_s[2]]\n",
      "pick_up(coords_m)\n",
      "place(coords_nextto_s)\n",
      "pick_up(coords_l)\n",
      "place(coords_nextto_m)\n",
      "115 th val pred: [0.5, 0.5, 0.5], [1.5, 2.5, 3.5]\n",
      "115 th val ref: coords = detect('small block')\n",
      "pick_up(coords)\n",
      "coords = detect('yellow box')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "116 th val pred: [0.5, -0.5, 0]\n",
      "116 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0] - 1, coords[1], coords[2] - 0.5]\n",
      "move_to(coords)\n",
      "117 th val pred: [30]\n",
      "117 th val ref: change_movement_speed(30)\n",
      "118 th val pred: [0.5, 0.5, -1]\n",
      "118 th val ref: coords = detect('monitor')\n",
      "coords = [coords[0], coords[1]-0.5, coords[2]]\n",
      "move_to(coords)\n",
      "119 th val pred: [0.5, 0.5, 1]\n",
      "119 th val ref: coords = detect('bright yellow box')\n",
      "move_to(coords)\n",
      "120 th val pred: [0,0,0] = detect('wall')\n",
      "120 th val ref: coords_table = detect('table')\n",
      "coords_wall = detect('wall')\n",
      "distance = np.sqrt(((coords_table[0]+coords_wall[0])/2)2 + ((coords_table[1]+coords_wall[1])/2)2 + ((coords_table[2]+coords_wall[2])/2)2)\n",
      "print(distance)\n",
      "121 th val pred: [0.5, 0.5, 0.5] = detect('green pen')\n",
      "[0.5, 0.5, 0.5] = detect('rear dark pencil')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "121 th val ref: coords = detect('green pen')\n",
      "pick_up(coords)\n",
      "coords = detect('rear dark pencil')\n",
      "place(coords)\n",
      "122 th val pred: [0.2, 0.7]\n",
      "122 th val ref: change_movement_speed(70)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "123 th val pred: [desk_x, desk_y, desk_z] = detect('desk')\n",
      "[window_x, window_y, window_z] = detect('window')\n",
      "\n",
      "distance = calculate_distance(desk_x, desk_y, desk_z, window_x, window_y, window_z)\n",
      "\n",
      "print(f\"The distance between the desk and the window is {distance} units.\")\n",
      "123 th val ref: coords_desk = detect('desk')\n",
      "coords_window = detect('window')\n",
      "distance = np.sqrt(((coords_desk[0]+coords_window[0])/2)2 + ((coords_desk[1]+coords_window[1])/2)2 + ((coords_desk[2]+coords_window[2])/2)2)\n",
      "print(distance)\n",
      "124 th val pred: [0.5, 1, 0]\n",
      "124 th val ref: coords = detect('clock')\n",
      "pick_up(coords)\n",
      "coords_wall = detect('wall')\n",
      "place(coords_wall)\n",
      "125 th val pred: [0.0, -1.5, 0.0], [0.0, 1.0, 0.0]\n",
      "125 th val ref: coords = detect('jacket')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 1.5, coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "126 th val pred: [0.5, 1, 0.5] = detect('large blue button')\n",
      "move_to(detect('large blue button'))\n",
      "rotate_screwer(detect('large blue button'))\n",
      "126 th val ref: coords = detect('large blue button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.08])\n",
      "change_movement_speed(70)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.08])\n",
      "127 th val pred: [0.5, 0.0, 0.0], [1.0, 0.0, 0.0]\n",
      "127 th val ref: coords = detect('phone')\n",
      "pick_up(coords)\n",
      "coords = detect('right blue bin')\n",
      "place(coords)\n",
      "128 th val pred: [0.5, 0.5, 0.5], [1.0, 0.0, 0.0]\n",
      "128 th val ref: coords = detect('pencil')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1], coords[2]-0.5]\n",
      "place(coords)\n",
      "129 th val pred: [0.5, 0.5, 0.5] = detect('black phone')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "129 th val ref: coords = detect('black phone')\n",
      "pick_up(coords)\n",
      "coords = detect('trash')\n",
      "place(coords)\n",
      "130 th val pred: [0.5, 1, 2]\n",
      "130 th val ref: coords = detect('largest screw on right')\n",
      "rotate_screwer(coords)\n",
      "131 th val pred: [0.5, 0.5, 0.5], [0.7, 0.5, 0.5]\n",
      "131 th val ref: coords = detect('red pen')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 1, coords[1], coords[2]]\n",
      "place(coords)\n",
      "132 th val pred: [0.5, 1, 2]\n",
      "132 th val ref: coords = detect('hammer')\n",
      "coords_b = detect('box')\n",
      "coords_s = detect('screwdriver')\n",
      "coords = [(coords_b[0]+coords_s[0])/2, (coords_b[1]+coords_s[1])/2, (coords_b[2]+coords_s[2])/2]\n",
      "place(coords)\n",
      "133 th val pred: [0, -8, 0]\n",
      "133 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]-0.08, coords[2]]\n",
      "move_to(coords)\n",
      "134 th val pred: [0.5, 1, 2]\n",
      "134 th val ref: coords = detect('pizza')\n",
      "pick_up(coords)\n",
      "coords = detect('oven')\n",
      "place(coords)\n",
      "135 th val pred: [0, 0, 1]\n",
      "135 th val ref: coords_cabinet = detect('cabinet')\n",
      "depth = coords_cabinet[2]\n",
      "print(depth)\n",
      "136 th val pred: [0.5, 0.5, 0.5] = detect(\"brown bin\")\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "136 th val ref: coords = detect('brown bin')\n",
      "move_to(coords)\n",
      "137 th val pred: [0.5, 1.0, 0.0] = detect(\"curtains\")\n",
      "[0.5, 1.0, 0.0] = detect(\"window\")\n",
      "rotate_screwer([0.5, 1.0, 0.0])\n",
      "move_to([0.5, 1.0, 0.0])\n",
      "137 th val ref: coords = detect('curtains')\n",
      "pick_up(coords)\n",
      "coords_window = detect('window')\n",
      "place(coords_window)\n",
      "138 th val pred: [0.75, -1.0, 0]\n",
      "138 th val ref: coords = detect('carpet')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 1.5, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "139 th val pred: [0.5, 1, 0], [0.5, 2, 0], [0.5, 3, 0], [0.5, 4, 0], [0.5, 5, 0], [0.5, 6, 0], [0.5, 7, 0], [0.5, 8, 0], [0.5, 9, 0], [0.5, 10, 0], [0.5, 11, 0], [0.5,\n",
      "139 th val ref: coords_shelf = detect('shelf')\n",
      "pick_up(coords_shelf)\n",
      "coords_floor = detect('floor')\n",
      "place(coords_floor)\n",
      "140 th val pred: [0.5, 0.5, 0.5], [1.0, 0.0, 0.0]\n",
      "140 th val ref: coords = detect('blue pen')\n",
      "pick_up(coords)\n",
      "coords = detect('green block')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "141 th val pred: [0.5, 0.5, 0.5] = detect('yellow pen')\n",
      "move_to(detect('yellow pen'))\n",
      "141 th val ref: coords = detect('yellow pen')\n",
      "move_to(coords)\n",
      "142 th val pred: [0.5, 0.5, 0.5] = detect('gray bin')\n",
      "142 th val ref: coords = detect('gray bin')\n",
      "move_to(coords)\n",
      "143 th val pred: [0.5, 0, 1], ['pick_up', 'green screw'], [0.5, 0, 2], ['place', 'hole on the wall']\n",
      "143 th val ref: print('Error: can not use screwer when using gripper')\n",
      "144 th val pred: [0.5, 0.5, 0.5] = detect(\"large red button\")\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "144 th val ref: coords = detect('large red button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.06])\n",
      "change_movement_speed(80)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.06])\n",
      "145 th val pred: [0.5, 1, 2]\n",
      "145 th val ref: coords = detect('book')\n",
      "pick_up(coords)\n",
      "coords = detect('shelf')\n",
      "place(coords)\n",
      "146 th val pred: [0.5, 0.5, 0.5], [1.0, 0.5, 0.5]\n",
      "146 th val ref: coords = detect('scissors')\n",
      "pick_up(coords)\n",
      "coords = detect('table')\n",
      "place(coords)\n",
      "147 th val pred: [move_to([-10,0,0]), rotate_screwer([-10,0,0]), place([-10,0,0])]\n",
      "147 th val ref: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] + 0.1, coords[1], coords[2]]\n",
      "place(coords)\n",
      "148 th val pred: [0.5, 1, 2]\n",
      "148 th val ref: coords = detect('pencil')\n",
      "print(coords)\n",
      "149 th val pred: [0.1, 0.0, 0.0]\n",
      "149 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]-0.05, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "150 th val pred: [0.5, 0.5, 0]\n",
      "150 th val ref: coords = detect('remote control')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 2, coords[1] - 1, coords[2]]\n",
      "place(coords)\n",
      "151 th val pred: [0.5, 0.5, 0.5] = detect('black pencil')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "151 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "152 th val pred: [0.5, 0.5, 0.5] = detect(\"green cabinet\")\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "152 th val ref: coords = detect('green cabinet')\n",
      "move_to(coords)\n",
      "153 th val pred: [0.5, 0.5, 0.5]\n",
      "153 th val ref: coords = detect('glue')\n",
      "print(coords)\n",
      "154 th val pred: [0,1,0],80\n",
      "154 th val ref: change_movement_speed(80)\n",
      "coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+2, coords[2]]\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "155 th val pred: [0.5, -10, 0]\n",
      "155 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0]+0.1, coords[1], coords[2]]\n",
      "move_to(coords)\n",
      "156 th val pred: [move_to([0,0,40])]\n",
      "156 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.4, coords[2]]\n",
      "move_to(coords)\n",
      "157 th val pred: [0.5, 1.0, 0.0] = detect('plate')\n",
      "place([0.5, 1.0, 0.0])\n",
      "157 th val ref: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "158 th val pred: [0.5, 0.5, 0]\n",
      "158 th val ref: coords = detect('camera')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 3, coords[1] + 1, coords[2]]\n",
      "place(coords)\n",
      "159 th val pred: [0.5, 0.5, 0.5] = detect('blue pen')\n",
      "move_to(detect('blue pen'))\n",
      "159 th val ref: coords = detect('blue pen')\n",
      "move_to(coords)\n",
      "160 th val pred: [0.5, 0.5, 0.5] = detect('rear red pencil')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "160 th val ref: coords = detect('rear red pencil')\n",
      "move_to(coords)\n",
      "161 th val pred: [0.5, 0.3, 0.2] = detect('screw')\n",
      "rotate_screwer([0.5, 0.3, 0.2])\n",
      "pick_up([0.5, 0.3, 0.2])\n",
      "move_to([0.5, 0.3, 0.4])\n",
      "place([0.5, 0.3, 0.4])\n",
      "161 th val ref: coords = detect('third smallest screw')\n",
      "rotate_screwer(coords)\n",
      "162 th val pred: [0.5, 0.5, 0.5] = detect('black folder')\n",
      "[0.5, 0.5, 0.5] = detect('stapler')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "162 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "163 th val pred: [0.5, 0.5, 0.5] = detect(\"orange pen\")\n",
      "move_to(detect(\"orange pen\"))\n",
      "163 th val ref: coords = detect('orange pen')\n",
      "move_to(coords)\n",
      "164 th val pred: [move_to([150,0,0]),change_movement_speed(20),wait(4),move_to([-150,0,0])]\n",
      "164 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.4, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(4)\n",
      "move_to(coords)\n",
      "165 th val pred: [move_to([0,0,-1]), rotate_screwer([0,0,90]), pick_up([0,0,0]), move_to([0,0,49]), place([0,0,0])]\n",
      "165 th val ref: coords = detect('eraser')\n",
      "pick_up(coords)\n",
      "coords = [coords[0], coords[1] + 0.5, coords[2]]\n",
      "place(coords)\n",
      "166 th val pred: [0.5, 1.0, 0.5], [0.75, 1.0, 0.5], [0.25, 1.0, 0.5]\n",
      "166 th val ref: coords = detect('orange block')\n",
      "pick_up(coords)\n",
      "coords = detect('blue cube')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "167 th val pred: [change_movement_speed(85)]\n",
      "167 th val ref: change_movement_speed(85)\n",
      "168 th val pred: [0.5, 1.0, 0.0]\n",
      "168 th val ref: coords = detect('window')\n",
      "move_to(coords)\n",
      "169 th val pred: [0.5, 1.0, 2.0] = detect('chair')\n",
      "rotate_screwer([0.5, 1.0, 2.0])\n",
      "move_to([0.5, 1.0, 3.0])\n",
      "pick_up([0.5, 1.0, 3.0])\n",
      "place([0.5, 1.0, 4.0])\n",
      "169 th val ref: coords = detect('chair')\n",
      "pick_up(coords)\n",
      "coords = detect('balcony')\n",
      "place(coords)\n",
      "170 th val pred: [0.5, 0.5, 0.5] = detect('front purple pen')\n",
      "move_to(detect('front purple pen'))\n",
      "170 th val ref: coords = detect('purple pen')\n",
      "move_to(coords)\n",
      "171 th val pred: [0.5, 1.0, 2.0]\n",
      "171 th val ref: coords = detect('printer')\n",
      "pick_up(coords)\n",
      "coords = detect('office')\n",
      "place(coords)\n",
      "172 th val pred: [0.5, 1.0, 2.0] = detect('wardrobe')\n",
      "rotate_screwer([0.5, 1.0, 2.0])\n",
      "move_to([0.5, 1.0, 3.0])\n",
      "change_movement_speed(80)\n",
      "pick_up([0.5, 1.0, 3.0])\n",
      "place([0.5, 1.0, 4.0])\n",
      "172 th val ref: coords_wardrobe = detect('wardrobe')\n",
      "coords_corner = detect('corner')\n",
      "place(coords_corner)\n",
      "173 th val pred: [0.5, 1.2, 0.8]\n",
      "173 th val ref: coords_bathtub = detect('bathtub')\n",
      "coords_shower = detect('shower')\n",
      "distance = np.sqrt(((coords_bathtub[0]+coords_shower[0])/2)2 + ((coords_bathtub[1]+coords_shower[1])/2)2 + ((coords_bathtub[2]+coords_shower[2])/2)2)\n",
      "print(distance)\n",
      "174 th val pred: [0.5, 0.5, 0.5] = detect('black pencil')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "174 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "175 th val pred: [move_to([1,0,0]),change_movement_speed(70),wait(5),move_to([-1,0,0])]\n",
      "175 th val ref: coords = check_current_pose()\n",
      "coords2 = [coords[0]-0.5, coords[1], coords[2]]\n",
      "move_to(coords2)\n",
      "time.sleep(5)\n",
      "move_to(coords)\n",
      "176 th val pred: [0.5, -1, 0], [0.5, 1, 0], [0.5, 0, 1]\n",
      "176 th val ref: coords = detect('sixth largest screw on left')\n",
      "rotate_screwer(coords)\n",
      "177 th val pred: [move_to([-0.5, 0, 1]), rotate_screwer([-0.5, 0, 1]), pick_up([-0.5, 0, 1]), move_to([0.5, 0, 1]), rotate_screwer([0.5, 0, 1]), place([0.5, 0, 1])]\n",
      "177 th val ref: coords = detect('left box')\n",
      "pick_up(coords)\n",
      "coords = detect('right blue bin')\n",
      "place(coords)\n",
      "178 th val pred: [move_to([0,0,60])]\n",
      "178 th val ref: coords = check_current_pose()\n",
      "coords = [coords[0], coords[1]+0.6, coords[2]]\n",
      "move_to(coords)\n",
      "179 th val pred: [0.5, 1.0, 0.0] = detect('plate')\n",
      "place([0.5, 1.0, 0.0])\n",
      "179 th val ref: coords = detect('plate')\n",
      "pick_up(coords)\n",
      "coords = detect('dishwasher')\n",
      "place(coords)\n",
      "180 th val pred: [0, 0, 1] = detect('floor')\n",
      "[0, 0, -1] = detect('ceiling')\n",
      "180 th val ref: coords_floor = detect('floor')\n",
      "coords_ceiling = detect('ceiling')\n",
      "distance = np.sqrt(((coords_floor[0]+coords_ceiling[0])/2)2 + ((coords_floor[1]+coords_ceiling[1])/2)2 + ((coords_floor[2]+coords_ceiling[2])/2)2)\n",
      "print(distance)\n",
      "181 th val pred: [0.5, 0.5, 0.5] = detect(\"green pen\")\n",
      "move_to(detect(\"green pen\"))\n",
      "181 th val ref: coords = detect('green pen')\n",
      "move_to(coords)\n",
      "182 th val pred: [7,0,0],rotate_screwer([7,0,0]),pick_up([7,0,0]),move_to([6,0,0]),change_movement_speed(100),place([6,0,0])\n",
      "182 th val ref: coords = detect('seventh largest screw')\n",
      "rotate_screwer(coords)\n",
      "183 th val pred: [0.5, 0.5, 0.5] = detect('small yellow button')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "183 th val ref: coords = detect('small yellow button')\n",
      "move_to([coords[0],coords[1],coords[2]+0.2])\n",
      "change_movement_speed(70)\n",
      "move_to(coords)\n",
      "change_movement_speed(100)\n",
      "move_to([coords[0],coords[1],coords[2]+0.2])\n",
      "184 th val pred: [0.5, 0.5, 0.5] = detect('black marker')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "184 th val ref: coords = detect('black marker')\n",
      "pick_up(coords)\n",
      "coords = detect('white paper')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "185 th val pred: [0.5, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.\n",
      "185 th val ref: coords = detect('yellow block')\n",
      "pick_up(coords)\n",
      "coords = detect('green cube')\n",
      "coords = [coords[0], coords[1], coords[2]+0.2]\n",
      "place(coords)\n",
      "186 th val pred: [0.5, 0.5, 0.5] = detect('red screw')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([1, 2, 3])\n",
      "pick_up([1, 2, 3])\n",
      "place([4, 5, 6])\n",
      "186 th val ref: print('Error: can not use screwer when using gripper')\n",
      "187 th val pred: [0.5, 0.5, 0.5], rotate_screwer([0.5, 0.5, 0.5]), move_to([0.5, 0.5, 0.5]), detect(\"red marker\"), move_to([0.5, 0.5, 0.5]), rotate_screwer([0.5, 0.5, 0.5]), move_to([0.5, 0.5, 0.5]), detect(\"white paper\"), move_to([0.5, 0.5, 0\n",
      "187 th val ref: coords = detect('red marker')\n",
      "pick_up(coords)\n",
      "coords = detect('white paper')\n",
      "coords = [coords[0], coords[1]+0.2, coords[2]]\n",
      "place(coords)\n",
      "188 th val pred: [0.5, 1, 2]\n",
      "188 th val ref: coords = detect('tenth largest screw')\n",
      "rotate_screwer(coords)\n",
      "189 th val pred: [0.5, 0.0, 0.0] = detect(\"tape\")\n",
      "place([0.5 + 0.08, 0.0, 0.0])\n",
      "189 th val ref: coords = detect('tape')\n",
      "pick_up(coords)\n",
      "coords = [coords[0] - 0.08, coords[1], coords[2]]\n",
      "place(coords)\n",
      "190 th val pred: [0.5, 1, 2]\n",
      "190 th val ref: coords = detect('toy car')\n",
      "print(coords)\n",
      "191 th val pred: [0.5, 1.0, 0.0] = detect('kettle')\n",
      "[0.5, 1.0, 0.0] = detect('sink')\n",
      "move_to([0.5, 1.0, 0.0])\n",
      "rotate_screwer([0.5, 1.0, 0.0])\n",
      "pick_up([0.5, 1.0, 0.0])\n",
      "place([0.5, 1.0, 0.0])\n",
      "191 th val ref: coords = detect('kettle')\n",
      "pick_up(coords)\n",
      "coords = detect('sink')\n",
      "place(coords)\n",
      "192 th val pred: [0.5, 0.5, 0.5] = detect('black pencil')\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "192 th val ref: coords = detect('rear black pencil')\n",
      "move_to(coords)\n",
      "193 th val pred: [10, 25, 30]\n",
      "193 th val ref: coords = detect('eleventh largest screw')\n",
      "rotate_screwer(coords)\n",
      "194 th val pred: [0.5, 0.5, 0.5] = detect('black folder')\n",
      "[0.5, 0.5, 0.5] = detect('stapler')\n",
      "move_to([0.5, 0.5, 0.5])\n",
      "rotate_screwer([0.5, 0.5, 0.5])\n",
      "pick_up([0.5, 0.5, 0.5])\n",
      "place([0.5, 0.5, 0.5])\n",
      "194 th val ref: print('Error: can not pick up more than one object at the same time')\n",
      "195 th val pred: [0.5, 0.5, 1]\n",
      "195 th val ref: coords = detect('marble shelf')\n",
      "move_to(coords)\n",
      "196 th val pred: [12, 'left', 'screw']\n",
      "196 th val ref: coords = detect('twelfth largest screw on left')\n",
      "rotate_screwer(coords)\n"
     ]
    }
   ],
   "source": [
    "for i, (val_pred, val_ref) in enumerate(zip(val_preds, val_refs)):\n",
    "    print(f'{i} th val pred:', val_pred)\n",
    "    print(f'{i} th val ref:', val_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bbce1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = rouge.compute(\n",
    "    predictions=val_preds,\n",
    "    references=val_refs,\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5da3ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.2095366259315898),\n",
       " 'rouge2': np.float64(0.09974649211433884),\n",
       " 'rougeL': np.float64(0.18851607400784096),\n",
       " 'rougeLsum': np.float64(0.20324492399778302)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b60c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
